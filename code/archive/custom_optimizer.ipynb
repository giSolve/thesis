{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Custom Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# imports \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from openTSNE import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing MNIST data \n",
    "X = np.zeros((x_train.shape[0], 784))\n",
    "for i in range(x_train.shape[0]):\n",
    "    X[i] = x_train[i].flatten()\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(y_train)\n",
    "\n",
    "# shuffle dataset and take random 20% for visualisation with tSNE \n",
    "X_sample = X.sample(frac=0.5, random_state=12).reset_index(drop=True)\n",
    "Y_sample = Y.sample(frac=0.5, random_state=12).reset_index(drop=True)\n",
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Finding 150 nearest neighbors using Annoy approximate search using euclidean distance...\n",
      "   --> Time elapsed: 59.02 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 2.64 seconds\n"
     ]
    }
   ],
   "source": [
    "# step 1: define affinities \n",
    "import openTSNE\n",
    "affinites = openTSNE.affinity.PerplexityBasedNN(\n",
    "    X_sample.to_numpy(),\n",
    "    perplexity=50, \n",
    "    n_jobs=-1,\n",
    "    random_state=12,\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = openTSNE.initialization.random(n_samples=len(X_sample) ,random_state=12, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = openTSNE.TSNEEmbedding(\n",
    "    init, \n",
    "    affinites, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_optimize(\n",
    "        self,\n",
    "        n_iter,\n",
    "        inplace=False,\n",
    "        propagate_exception=False,\n",
    "        **gradient_descent_params,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        TSNEEmbedding\n",
    "            An optimized t-SNE embedding.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        OptimizationInterrupt\n",
    "            If a callback stops the optimization and the ``propagate_exception``\n",
    "            flag is set, then an exception is raised.\n",
    "\n",
    "        \"\"\"\n",
    "        # Typically we want to return a new embedding and keep the old one intact\n",
    "        if inplace:\n",
    "            embedding = self\n",
    "        else:\n",
    "            embedding = TSNEEmbedding(\n",
    "                np.copy(self),\n",
    "                self.affinities,\n",
    "                random_state=self.random_state,\n",
    "                optimizer=self.optimizer.copy(),\n",
    "                **self.gradient_descent_params,\n",
    "            )\n",
    "\n",
    "        # If optimization parameters were passed to this funciton, prefer those\n",
    "        # over the defaults specified in the TSNE object\n",
    "        optim_params = dict(self.gradient_descent_params)\n",
    "        optim_params.update(gradient_descent_params)\n",
    "        optim_params[\"n_iter\"] = n_iter\n",
    "        _handle_nice_params(embedding, optim_params)\n",
    "\n",
    "        try:\n",
    "            # Run gradient descent with the embedding optimizer so gains are\n",
    "            # properly updated and kept\n",
    "            error, embedding = embedding.optimizer(\n",
    "                embedding=embedding, P=self.affinities.P, **optim_params\n",
    "            )\n",
    "\n",
    "        except OptimizationInterrupt as ex:\n",
    "            log.info(\"Optimization was interrupted with callback.\")\n",
    "            if propagate_exception:\n",
    "                raise ex\n",
    "            error, embedding = ex.error, ex.final_embedding\n",
    "\n",
    "        embedding.kl_divergence = error\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_embedding = embedding.optimize(n_iter=250, learning_rate=200, exaggeration=12, momentum=0.5, callbacks= kld_tracker_EE, callbacks_every_iters=10, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wissrech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
