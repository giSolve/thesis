{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Automated Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 10:06:26.671545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "from openTSNE import TSNE\n",
    "import openTSNE\n",
    "import time\n",
    "import json\n",
    "from math import ceil\n",
    "\n",
    "# for local imports  \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os \n",
    "\n",
    "script_dir = Path.cwd().parent / \"scripts\"\n",
    "sys.path.append(str(script_dir))\n",
    "\n",
    "figures_dir = Path.cwd().parent / \"figures\"\n",
    "sys.path.append(str(figures_dir))\n",
    "\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "sys.path.append(str(results_dir))\n",
    "\n",
    "import datasets\n",
    "import quality_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KLD and KLDRC Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import callbacks \n",
    "class KLDMonitor(callbacks.Callback):\n",
    "    def __init__(self, record_every=5):\n",
    "        self.record_every = record_every\n",
    "        self.kl_divergences = {}\n",
    "    \n",
    "    def __call__(self, iteration, error, embedding):\n",
    "        \"\"\"\n",
    "        Monitors KL divergence while ensuring unique iteration values.\n",
    "        \"\"\"\n",
    "        # Only check every `record_every` iterations\n",
    "        if iteration % self.record_every == 0:\n",
    "            # in this case we are in the embedding phase  \n",
    "            if iteration in self.kl_divergences or iteration > 250:\n",
    "                self.kl_divergences[iteration + 250] = error \n",
    "            # in the EE phase     \n",
    "            else: \n",
    "                self.kl_divergences[iteration] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_ee = 15\n",
    "switch_buffer = 2 \n",
    "\n",
    "class KLDRCMonitorEE(callbacks.Callback):\n",
    "    def __init__(self, record_every=3):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            record_every (int): Check KL divergence every this many iterations.\n",
    "            buffer_ee (int): Minimum iterations before monitoring KL divergence.\n",
    "            switch_buffer (int): Extra iterations to confirm EE phase exit.\n",
    "        \"\"\"\n",
    "        self.record_every = record_every  # Equivalent to `auto_iter_pollrate_ee = 3`\n",
    "        self.kl_divergences = []\n",
    "        self.rel_changes = [] # this should be a list with entries (iteration, rel_change)\n",
    "        self.last_error = None\n",
    "        self.last_rel_change = None\n",
    "        self.switch_buffer_count = switch_buffer  # Tracks remaining iterations before exiting EE\n",
    "\n",
    "    def __call__(self, iteration, error, embedding):\n",
    "        \"\"\"\n",
    "        Monitors KL divergence and determines when to stop Early Exaggeration.\n",
    "        Returns True if EE should stop.\n",
    "        \"\"\"\n",
    "        # Only check every `record_every` iterations\n",
    "        if iteration % self.record_every == 0:\n",
    "            self.kl_divergences.append((iteration, error))\n",
    "\n",
    "            if self.last_error is not None:\n",
    "                # Compute relative change: (prev_error - current_error) / prev_error\n",
    "                rel_change = 100 * (self.last_error - error) / self.last_error  \n",
    "                self.rel_changes.append((iteration, rel_change))\n",
    "\n",
    "                print(f\"Iteration {iteration}: KL Divergence = {error:.4f}, Relative Change = {rel_change:.4f}%\")\n",
    "\n",
    "                # Start checking only after `buffer_ee` iterations\n",
    "                if iteration > buffer_ee:\n",
    "                    if self.last_rel_change is not None and rel_change < self.last_rel_change:\n",
    "                        # If relative change decreases, start the switch buffer countdown\n",
    "                        if self.switch_buffer_count < 1:\n",
    "                            print(\"Relative change has consistently decreased. Stopping Early Exaggeration.\")\n",
    "                            print(f\"EE Iteration stopped at {iteration}\")\n",
    "                            # Signal to stop EE phase, we return iteration as well \n",
    "                            return True, iteration  \n",
    "                        self.switch_buffer_count -= 1\n",
    "                    else:\n",
    "                        # Reset switch buffer if relative change increases again\n",
    "                        self.switch_buffer_count = switch_buffer\n",
    "\n",
    "                self.last_rel_change = rel_change\n",
    "\n",
    "            # Update last error for the next iteration\n",
    "            self.last_error = error\n",
    "\n",
    "        return False  # Continue EE phase if conditions are not met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_run = 150 \n",
    "auto_iter_end = 100 \n",
    "\n",
    "class KLDRCMonitorRun(callbacks.Callback):\n",
    "    def __init__(self, record_every=5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            record_every (int): Check KL divergence every this many iterations.\n",
    "            buffer_run (int): Minimum iterations after EE before monitoring for stopping.\n",
    "            auto_iter_end (float): Threshold for stopping, lower values stop earlier.\n",
    "        \"\"\"\n",
    "        self.record_every = record_every  # Equivalent to `auto_iter_pollrate_run = 5`\n",
    "        # self.buffer_run = buffer_run  # Equivalent to `auto_iter_buffer_run = 15`\n",
    "        # self.auto_iter_end = auto_iter_end  # Used for stopping condition\n",
    "\n",
    "        self.kl_divergences = []\n",
    "        self.last_error = None\n",
    "\n",
    "    def __call__(self, iteration, error, embedding):\n",
    "        \"\"\"\n",
    "        Monitors KL divergence and determines when to stop the full t-SNE run.\n",
    "        Returns True if the run should stop.\n",
    "        \"\"\"\n",
    "        # Only check KL divergence every `record_every` iterations\n",
    "        if iteration % self.record_every == 0:\n",
    "            self.kl_divergences.append((iteration, error))\n",
    "\n",
    "            if self.last_error is not None:\n",
    "                # Compute absolute error difference\n",
    "                error_diff = abs(self.last_error - error)\n",
    "\n",
    "                print(f\"Iteration {iteration}: KL Divergence = {error:.4f}, Error Diff = {error_diff:.6f}\")\n",
    "\n",
    "                # Start monitoring only after `buffer_run` iterations have passed\n",
    "                if iteration > buffer_run:\n",
    "                    # Stopping condition from C++: abs(error_diff)/pollrate < error/auto_iter_end\n",
    "                    if (error_diff / self.record_every) < (error / auto_iter_end):\n",
    "                        print(\"KL divergence change is below threshold. Stopping optimization.\")\n",
    "                        print(f\"Run iteration stopped at {iteration}\")\n",
    "                        return True, iteration  # Signal to stop t-SNE run\n",
    "\n",
    "            # Update last error\n",
    "            self.last_error = error\n",
    "\n",
    "        return False  # Continue t-SNE run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optSNE(data, labels, random_state):\n",
    "    \"\"\"\n",
    "    Runs the optimized t-SNE (i.e. with automated stopping) on a given dataset.\n",
    "    This version accepts data and labels as inputs (instead of using a global DataFrame).\n",
    "    \n",
    "    Returns:\n",
    "      embedding: the final embedding.\n",
    "      final_kld: final KL divergence value.\n",
    "      labels: the labels (unchanged).\n",
    "      kld_monitor: the monitor object containing the KL divergence history.\n",
    "    \"\"\"\n",
    "    # In this version we use the entire dataset.\n",
    "    features = data  # assuming data is already a numpy array\n",
    "    \n",
    "    initiali = openTSNE.initialization.pca(features, random_state=random_state)\n",
    "    \n",
    "    # using default perplexity\n",
    "    affinities = openTSNE.affinity.PerplexityBasedNN(\n",
    "        features,\n",
    "        perplexity=30, \n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    embedding_obj = openTSNE.TSNEEmbedding(\n",
    "        initiali, \n",
    "        affinities, \n",
    "        random_state=random_state,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # --- Early Exaggeration phase ---\n",
    "    kld_monitor_EE = KLDRCMonitorEE(record_every=3)\n",
    "    try:\n",
    "        embedding_obj = embedding_obj.optimize(\n",
    "            n_iter=1000, \n",
    "            callbacks=kld_monitor_EE, \n",
    "            callbacks_every_iters=3, \n",
    "            verbose=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Early Exaggeration phase stopped early:\", e)\n",
    "\n",
    "    # extract results \n",
    "    ee_kld = kld_monitor_EE.kl_divergences \n",
    "    ee_rel = kld_monitor_EE.rel_changes \n",
    "    ee_offset = ee_kld[-1][0] if ee_kld else 0\n",
    "    \n",
    "    # --- Embedding phase ---\n",
    "    kld_tracker_embed = KLDRCMonitorRun(record_every=5)\n",
    "    try:\n",
    "        embedding_obj = embedding_obj.optimize(\n",
    "            n_iter=1000,\n",
    "            callbacks=kld_tracker_embed, \n",
    "            callbacks_every_iters=5, \n",
    "            verbose=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Embedding phase stopped early:\", e)\n",
    "\n",
    "    # kld offset for plotting reasons\n",
    "    run_kld = [(iteration + ee_offset, error) for (iteration, error) in kld_tracker_embed.kl_divergences]\n",
    "    combined_kld = ee_kld + run_kld\n",
    "        \n",
    "    monitor_data = {\n",
    "        \"kl_divergences\": combined_kld,\n",
    "        \"rel_changes\": ee_rel\n",
    "    }\n",
    "    return embedding_obj, labels, monitor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne_experiments(datasets, seeds, default_EE=250, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs both normal t-SNE and optimized t-SNE (opt-SNE) on each dataset,\n",
    "    each with multiple seeds.\n",
    "    \n",
    "    Parameters:\n",
    "      datasets: list of tuples (data, labels)\n",
    "      seeds: list of seeds (e.g., 3 seeds)\n",
    "      default_EE: fixed early exaggeration iterations for normal t-SNE\n",
    "      verbose: verbosity flag\n",
    "      \n",
    "    Returns:\n",
    "      embeddings: dict with keys ((dataset_index, setting), seed) mapping to \n",
    "                  (embedding, labels, kld_history)\n",
    "      timings: list of dicts with timing info.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    timings = []\n",
    "    \n",
    "    for d_idx, (data, labels) in enumerate(datasets):\n",
    "        if hasattr(data, \"values\"):\n",
    "            data = data.values.astype(float)\n",
    "\n",
    "        for seed in seeds:\n",
    "            # --- Run normal t-SNE with default settings ---\n",
    "            # using normal kld monitor\n",
    "            kld_monitor_norm = KLDMonitor()  \n",
    "            tsne_norm = TSNE(early_exaggeration_iter=default_EE, \n",
    "                             n_iter=750 - default_EE, \n",
    "                             n_jobs=-1,\n",
    "                             callbacks=kld_monitor_norm, \n",
    "                             callbacks_every_iters=5,\n",
    "                             random_state=seed, \n",
    "                             verbose=verbose)\n",
    "            t0 = time.time()\n",
    "            embedding_norm = tsne_norm.fit(data)\n",
    "            elapsed_normal = time.time() - t0\n",
    "            kld_norm = kld_monitor_norm.kl_divergences\n",
    "            embeddings[((d_idx, \"normal\"), seed)] = (embedding_norm, labels, kld_norm)\n",
    "            timings.append({\n",
    "                \"dataset_index\": d_idx, \n",
    "                \"setting\": \"normal\", \n",
    "                \"seed\": seed, \n",
    "                \"time_taken_seconds\": elapsed_normal\n",
    "            })\n",
    "            \n",
    "            # --- Run optimized t-SNE (opt-SNE) ---\n",
    "            t0 = time.time()\n",
    "            embedding_opt, labels_opt, monitor_data = run_optSNE(\n",
    "                data, labels, random_state=seed)\n",
    "            elapsed_opt = time.time() - t0\n",
    "        \n",
    "            embeddings[((d_idx, \"opt\"), seed)] = (embedding_opt, labels_opt, monitor_data)\n",
    "            timings.append({\n",
    "                \"dataset_index\": d_idx, \n",
    "                \"setting\": \"opt\", \n",
    "                \"seed\": seed, \n",
    "                \"time_taken_seconds\": elapsed_opt\n",
    "            })\n",
    "            \n",
    "    # Optionally, save timings to a file.\n",
    "    save_path = os.path.join(results_dir, \"opt-SNE_times.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(timings, f, indent=4)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Grid Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding_grid_experiments(embeddings, seeds, dataset_ids, file_start=\"opt-SNE\", cmap=\"tab20\"):\n",
    "    \"\"\"\n",
    "    Plots a grid of t-SNE embeddings from experiments.\n",
    "    Rows correspond to seeds and columns correspond to each (dataset, setting) combination.\n",
    "    Column order: for each dataset, first \"normal\" then \"opt\".\n",
    "    \n",
    "    Parameters:\n",
    "      embeddings (dict): Dictionary with keys ((dataset_index, setting), seed) mapping to \n",
    "                         (embedding, labels, kld_history).\n",
    "      seeds (list): List of seeds used.\n",
    "      dataset_ids (list): List of dataset indices or names (hashable).\n",
    "      file_start (str): Prefix for the saved filename.\n",
    "      cmap (str): Colormap to use.\n",
    "    \"\"\"\n",
    "    # Build column parameter list: one entry per (dataset, setting) combination.\n",
    "    column_parameters = []\n",
    "    for d in dataset_ids:\n",
    "        column_parameters.append((d, \"normal\"))\n",
    "        column_parameters.append((d, \"opt\"))\n",
    "        \n",
    "    row_parameters = seeds  # each row is one seed\n",
    "    n_rows = len(row_parameters)\n",
    "    n_cols = len(column_parameters)\n",
    "    \n",
    "    subplot_size = 2\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, subplot_size * n_rows), squeeze=False)\n",
    "    # fig.suptitle(\"t-SNE Embeddings\", fontsize=12)\n",
    "    \n",
    "    for r_idx, seed in enumerate(row_parameters):\n",
    "        for c_idx, col in enumerate(column_parameters):\n",
    "            ax = axes[r_idx, c_idx]\n",
    "            key = (col, seed)\n",
    "            if key not in embeddings:\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "            embedding, labels, _ = embeddings[key]\n",
    "            ax.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap=cmap, s=2, alpha=0.6)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            # For the top row, add a title showing dataset and setting.\n",
    "            if r_idx == 0:\n",
    "                d_idx, setting = col\n",
    "                ax.set_title(f\"{setting}\", fontsize=12, pad=10)\n",
    "            # For the left-most column, add a label showing the seed.\n",
    "            #if c_idx == 0:\n",
    "            #    ax.set_ylabel(f\"Seed: {seed}\", fontsize=12, labelpad=10)\n",
    "                \n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2, top=0.93)\n",
    "    save_path = os.path.join(figures_dir, f\"{file_start}_embedding_grid_{cmap}.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Quality Measures (without KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quality_measures(quality_results, dataset_names, plot_name=\"ee_length\", x_axis_label=\"EE length\", x_axis_entries=None):\n",
    "    \"\"\"\n",
    "    Plots three side-by-side plots for the three embedding quality measures.\n",
    "\n",
    "    Parameters:\n",
    "        quality_results (dict): Maps x-axis entries to dataset indices with tuples (mnn, mnn_global, rho).\n",
    "        dataset_names (dict): Maps dataset indices to dataset names.\n",
    "        plot_name (str): For saving the file, e.g. \"perp\", \"n_iter\" etc.\n",
    "        x_axis_label (str): Label for the x-axis.\n",
    "        x_axis_entries (list): Custom x-axis entries; defaults to sorted keys of quality_results (this should be okay).\n",
    "    \"\"\"\n",
    "    if x_axis_entries is None:\n",
    "        x_axis_entries = list(quality_results.keys())\n",
    "\n",
    "    dataset_indices = sorted(next(iter(quality_results.values())).keys())  # Extract dataset indices\n",
    "    dataset_data = {d: {'mnn': [], 'mnn_global': [], 'rho': []} for d in dataset_indices}\n",
    "\n",
    "    # Extract data\n",
    "    for x in x_axis_entries:\n",
    "        for d in dataset_indices:\n",
    "            mnn, mnn_global, rho = quality_results[x][d]\n",
    "            dataset_data[d]['mnn'].append(mnn)\n",
    "            dataset_data[d]['mnn_global'].append(mnn_global)\n",
    "            dataset_data[d]['rho'].append(rho)\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    titles = [\"KNN\", \"KNC\", \"CPD\"]\n",
    "\n",
    "    for i, (metric, title) in enumerate(zip(['mnn', 'mnn_global', 'rho'], titles)):\n",
    "        for d, data in dataset_data.items():\n",
    "            axes[i].plot(x_axis_entries, data[metric], marker='o', linestyle='-', label=dataset_names.get(d, f\"{d}\"))\n",
    "        axes[i].set_title(title, fontsize=12)\n",
    "        axes[i].set_xlabel(x_axis_label, fontsize=12)\n",
    "\n",
    "    # Formatting\n",
    "    for ax in axes:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=10, width=0.5)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8, width=0.3)\n",
    "        ax.spines[\"bottom\"].set_linewidth(0.5)\n",
    "        ax.spines[\"left\"].set_linewidth(0.5)\n",
    "\n",
    "    axes[0].legend(fontsize=10)  # Only first plot has legend\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save & Show\n",
    "    save_path = os.path.join(figures_dir, f\"{plot_name}_3_quality_measures.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot KL Divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kld_monitors_grid_experiments(embeddings, datasets, seed_for_kld=1, file_start=\"kld_opt\", mode=\"opt\"):\n",
    "    \"\"\"\n",
    "    Plots, for each dataset, the KL divergence and relative change curves (from opt-SNE)\n",
    "    for a specified seed (default seed 1), arranged in a single row (one subplot per dataset).\n",
    "    \n",
    "    Parameters:\n",
    "      embeddings (dict): Dictionary with keys ((dataset_index, setting), seed) mapping to \n",
    "                         (embedding, labels, kld_history).\n",
    "      datasets (list): List of dataset indices.\n",
    "      seed_for_kld: The seed to use for plotting (default 1).\n",
    "      file_start (str): Prefix for file saving.\n",
    "      mode (str): Which setting to use (e.g. \"opt\").\n",
    "    \"\"\"\n",
    "    n_datasets = len(datasets)\n",
    "    fig, axes = plt.subplots(1, n_datasets, figsize=(12, 4))\n",
    "    if n_datasets == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, d in enumerate(datasets):\n",
    "        key = ((d, mode), seed_for_kld)\n",
    "        if key not in embeddings:\n",
    "            axes[i].axis(\"off\")\n",
    "            continue\n",
    "        # Assume kld_history is a list of tuples.\n",
    "        # For this plot we assume each tuple is (iteration, kld_error, relative_change).\n",
    "        _, _, kld_history = embeddings[key]\n",
    "        iterations, kld_values, rel_changes = [], [], []\n",
    "        for record in kld_history:\n",
    "            # If the record has three entries, unpack iteration, error, rel_change.\n",
    "            if len(record) >= 3:\n",
    "                it, err, rel = record[:3]\n",
    "            else:\n",
    "                it, err = record[:2]\n",
    "                rel = 0  # or compute relative change if desired\n",
    "            iterations.append(it)\n",
    "            kld_values.append(err)\n",
    "            rel_changes.append(rel)\n",
    "        # Plot both curves.\n",
    "        axes[i].plot(iterations, kld_values, linestyle='-', marker='', color='blue', label=\"KLD\")\n",
    "        axes[i].plot(iterations, rel_changes, linestyle='-', marker='', color='red', label=\"Rel Change\")\n",
    "        axes[i].set_xlabel(\"Iteration\", fontsize=12)\n",
    "        axes[i].set_ylabel(\"Value\", fontsize=12)\n",
    "        axes[i].set_title(f\"Dataset {d} (opt, seed {seed_for_kld})\", fontsize=12)\n",
    "        axes[i].spines[\"top\"].set_visible(False)\n",
    "        axes[i].spines[\"right\"].set_visible(False)\n",
    "        axes[i].tick_params(axis=\"both\", which=\"major\", labelsize=10, width=0.5)\n",
    "        axes[i].tick_params(axis=\"both\", which=\"minor\", labelsize=8, width=0.3)\n",
    "        if i == 0:\n",
    "            axes[i].legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(figures_dir, f\"{file_start}_kl_divergences_grid.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kl_divergences_grid(embedding_dict, file_start):\n",
    "    \"\"\"\n",
    "    Plots averaged KL divergence curves (in this case, single-run curves) for each learning rate,\n",
    "    separately for each dataset in a 2x2 grid.\n",
    "\n",
    "    For each dataset:\n",
    "      - Extracts the KL divergence records (a dict mapping iteration -> error) for each learning rate.\n",
    "      - Sorts the learning rates (numeric ones first, then any non-numeric).\n",
    "      - Plots the KL divergence curve for each learning rate with the legend label using the Greek letter η.\n",
    "    \n",
    "    Parameters:\n",
    "      embedding_dict (dict): Dictionary with keys (η, dataset_index) mapping to \n",
    "                             (embedding, labels, kld_values) where kld_values is a dict mapping iteration -> error.\n",
    "      file_start (str): e.g eta, ee etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine unique dataset indices.\n",
    "    datasets = sorted({ds for (_, ds) in embedding_dict.keys()})\n",
    "    \n",
    "    # Optional: map dataset indices to names.\n",
    "    dataset_names = {0: \"Iris\", 1: \"Macosko\", 2: \"MNIST\", 3: \"Flow18\"}\n",
    "    \n",
    "    # Create a 2x2 grid of subplots.\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over each dataset to plot its KL divergence curves.\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Collect learning rates for this dataset.\n",
    "        ee_factors = [alpha for (alpha, ds) in embedding_dict.keys() if ds == dataset]\n",
    "        # Order learning rates: numeric first (sorted), then any non-numeric - this isn't strictly necessary\n",
    "        #numeric_lrs = sorted([eta for eta in lrs if isinstance(eta, (int, float))])\n",
    "        #non_numeric_lrs = [eta for eta in lrs if not isinstance(eta, (int, float))]\n",
    "        #ordered_lrs = numeric_lrs + non_numeric_lrs\n",
    "        \n",
    "        # Plot each learning rate's KL divergence curve.\n",
    "        for alpha in ee_factors:\n",
    "            _, _, kld_values = embedding_dict[(alpha, dataset)]\n",
    "            # Sort iterations and get corresponding error values.\n",
    "            iterations = sorted(kld_values.keys())\n",
    "            errors = [kld_values[it] for it in iterations]\n",
    "            ax.plot(iterations, errors, linestyle='-', label=f\"EE length = {alpha}\")\n",
    "        \n",
    "        # Format the subplot.\n",
    "        ax.set_xlabel(\"Iteration\", fontsize=12)\n",
    "        ax.set_ylabel(\"KL Divergence\", fontsize=12)\n",
    "        title = dataset_names.get(dataset, str(dataset))\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=10, width=0.5)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8, width=0.3)\n",
    "        \n",
    "        if idx==0: \n",
    "          ax.legend(fontsize=10)\n",
    "    \n",
    "    # Hide any unused subplots (if fewer than 4 datasets).\n",
    "    for j in range(len(datasets), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    # Adjust spacing between subplots.\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    \n",
    "    # Save and display the figure.\n",
    "    save_path = os.path.join(figures_dir, f\"{file_start}_kl_divergences_grid.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Plotting for Different Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_quality_measures(embedding_dict, X, ee_lengths):\n",
    "    \"\"\"\n",
    "    Computes the average quality measures across different seeds for each learning rate\n",
    "    and stores individual seed results.\n",
    "\n",
    "    Parameters:\n",
    "        embedding_dict (dict): Dictionary with (ee_iter, seed) as keys and\n",
    "                               (embedding, labels, kld_values) as values.\n",
    "        X (numpy.ndarray): Original high-dimensional data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with learning_rate as keys and values being another dictionary:\n",
    "              {\n",
    "                  \"average\": (mnn_avg, mnn_global_avg, rho_avg),\n",
    "                  \"seeds\": {seed: (mnn, mnn_global, rho), ...}\n",
    "              }\n",
    "    \"\"\"\n",
    "    quality_results = {}\n",
    "\n",
    "    for ee_len in ee_lengths:\n",
    "        mnn_list, mnn_global_list, rho_list = [], [], []\n",
    "        seed_results = {}\n",
    "\n",
    "        for seed in [key[1] for key in embedding_dict.keys() if key[0] == ee_len]:\n",
    "            # Unpack three elements; ignore kld_values\n",
    "            embedding, labels, _ = embedding_dict[(ee_len, seed)]\n",
    "            mnn, mnn_global, rho = quality_measures.embedding_quality(X, embedding, labels)\n",
    "\n",
    "            mnn_list.append(mnn)\n",
    "            mnn_global_list.append(mnn_global)\n",
    "            rho_list.append(rho)\n",
    "\n",
    "            # Store per-seed results\n",
    "            seed_results[seed] = (mnn, mnn_global, rho)\n",
    "\n",
    "        # Compute averages\n",
    "        quality_results[ee_len] = {\n",
    "            \"average\": (\n",
    "                np.mean(mnn_list),\n",
    "                np.mean(mnn_global_list),\n",
    "                np.mean(rho_list)\n",
    "            ),\n",
    "            \"seeds\": seed_results\n",
    "        }\n",
    "\n",
    "    return quality_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quality_measures_seeds(quality_results, x_axis_label=\"EE length\", plot_name=\"ee_length_seeds\"):\n",
    "    \"\"\"\n",
    "    Plots three side-by-side plots for the three embedding quality measures.\n",
    "    Includes individual seed results as connected grey lines and averaged results in black.\n",
    "\n",
    "    Parameters:\n",
    "        quality_results (dict): Dictionary with ee_length as keys and values being another dictionary:\n",
    "                                {\n",
    "                                    \"average\": (mnn_avg, mnn_global_avg, rho_avg),\n",
    "                                    \"seeds\": {seed: (mnn, mnn_global, rho), ...}\n",
    "                                }\n",
    "        x_axis_label (str): Label for the x-axis (default \"EE length\").\n",
    "        plot_name (str): Name for saving the file.\n",
    "        figures_dir (str): Directory where the plot will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get sorted EE lengths and create x-axis labels.\n",
    "    ee_lengths = sorted(quality_results.keys())\n",
    "    ee_labels = [str(x) for x in ee_lengths]\n",
    "\n",
    "    # Prepare averaged data for plotting.\n",
    "    mnn_values = [quality_results[x][\"average\"][0] for x in ee_lengths]\n",
    "    mnn_global_values = [quality_results[x][\"average\"][1] for x in ee_lengths]\n",
    "    rho_values = [quality_results[x][\"average\"][2] for x in ee_lengths]\n",
    "\n",
    "    # Prepare per-seed data: For each seed, collect quality measures across EE lengths.\n",
    "    seed_data = {}  # key: seed, value: dict with lists for each measure.\n",
    "    for x in ee_lengths:\n",
    "        seed_results = quality_results[x][\"seeds\"]\n",
    "        for seed, (mnn, mnn_global, rho) in seed_results.items():\n",
    "            if seed not in seed_data:\n",
    "                seed_data[seed] = {'mnn': [], 'mnn_global': [], 'rho': []}\n",
    "            seed_data[seed]['mnn'].append(mnn)\n",
    "            seed_data[seed]['mnn_global'].append(mnn_global)\n",
    "            seed_data[seed]['rho'].append(rho)\n",
    "\n",
    "    # Create the 1x3 subplot layout.\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    titles = [\"KNN\", \"KNC\", \"CPD\"]\n",
    "\n",
    "    # Plot per-seed results (grey lines) for each quality measure.\n",
    "    for seed, data in seed_data.items():\n",
    "        axes[0].plot(ee_labels, data['mnn'], linestyle='-', color='grey', alpha=0.5)\n",
    "        axes[1].plot(ee_labels, data['mnn_global'], linestyle='-', color='grey', alpha=0.5)\n",
    "        axes[2].plot(ee_labels, data['rho'], linestyle='-', color='grey', alpha=0.5)\n",
    "\n",
    "    # Plot averaged results (black with markers) for each quality measure.\n",
    "    axes[0].plot(ee_labels, mnn_values, marker='o', linestyle='-', color='black', label=\"Average\")\n",
    "    axes[1].plot(ee_labels, mnn_global_values, marker='o', linestyle='-', color='black', label=\"Average\")\n",
    "    axes[2].plot(ee_labels, rho_values, marker='o', linestyle='-', color='black', label=\"Average\")\n",
    "\n",
    "    # Set x-axis and y-axis labels.\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(x_axis_label, fontsize=12)\n",
    "    #axes[0].set_ylabel(\"KNN\", fontsize=12)\n",
    "    #axes[1].set_ylabel(\"KNC\", fontsize=12)\n",
    "    #axes[2].set_ylabel(\"CPD\", fontsize=12)\n",
    "\n",
    "    # Set subplot titles.\n",
    "    for i, title in enumerate(titles):\n",
    "        axes[i].set_title(title, fontsize=12)\n",
    "\n",
    "    # Formatting: remove top/right spines and adjust tick parameters.\n",
    "    for ax in axes:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=10, width=0.5)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8, width=0.3)\n",
    "        ax.spines[\"bottom\"].set_linewidth(0.5)\n",
    "        ax.spines[\"left\"].set_linewidth(0.5)\n",
    "\n",
    "    # Only include legend in the first plot.\n",
    "    axes[0].legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and display the figure.\n",
    "    save_path = os.path.join(figures_dir, f\"{plot_name}_3_quality_measures.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datasets.load_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing a smaller number of datapoints \n",
    "n_points = 200\n",
    "all_data = [datasets.load_n_samples(n_points, X, y) for (X, y) in all_data[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 12, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.02 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.6114, Relative Change = -0.2111%\n",
      "Iteration 9: KL Divergence = 1.3224, Relative Change = 17.9322%\n",
      "Iteration 12: KL Divergence = 0.7190, Relative Change = 45.6279%\n",
      "Iteration 15: KL Divergence = 0.4734, Relative Change = 34.1643%\n",
      "Iteration 18: KL Divergence = 0.4025, Relative Change = 14.9805%\n",
      "Iteration 21: KL Divergence = 0.3805, Relative Change = 5.4597%\n",
      "Iteration 24: KL Divergence = 0.3672, Relative Change = 3.4864%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.3506, Error Diff = 0.006374\n",
      "Iteration 15: KL Divergence = 0.3462, Error Diff = 0.004414\n",
      "Iteration 20: KL Divergence = 0.3441, Error Diff = 0.002126\n",
      "Iteration 25: KL Divergence = 0.3431, Error Diff = 0.001027\n",
      "Iteration 30: KL Divergence = 0.3420, Error Diff = 0.001028\n",
      "Iteration 35: KL Divergence = 0.3419, Error Diff = 0.000192\n",
      "Iteration 40: KL Divergence = 0.3412, Error Diff = 0.000678\n",
      "Iteration 45: KL Divergence = 0.3403, Error Diff = 0.000918\n",
      "Iteration 50: KL Divergence = 0.3394, Error Diff = 0.000843\n",
      "Iteration   50, KL divergence 0.3394, 50 iterations in 0.4529 sec\n",
      "Iteration 55: KL Divergence = 0.3391, Error Diff = 0.000286\n",
      "Iteration 60: KL Divergence = 0.3385, Error Diff = 0.000578\n",
      "Iteration 65: KL Divergence = 0.3383, Error Diff = 0.000257\n",
      "Iteration 70: KL Divergence = 0.3377, Error Diff = 0.000577\n",
      "Iteration 75: KL Divergence = 0.3377, Error Diff = 0.000002\n",
      "Iteration 80: KL Divergence = 0.3374, Error Diff = 0.000352\n",
      "Iteration 85: KL Divergence = 0.3373, Error Diff = 0.000019\n",
      "Iteration 90: KL Divergence = 0.3377, Error Diff = 0.000332\n",
      "Iteration 95: KL Divergence = 0.3373, Error Diff = 0.000369\n",
      "Iteration 100: KL Divergence = 0.3366, Error Diff = 0.000662\n",
      "Iteration  100, KL divergence 0.3366, 50 iterations in 0.4259 sec\n",
      "Iteration 105: KL Divergence = 0.3365, Error Diff = 0.000169\n",
      "Iteration 110: KL Divergence = 0.3366, Error Diff = 0.000075\n",
      "Iteration 115: KL Divergence = 0.3365, Error Diff = 0.000062\n",
      "Iteration 120: KL Divergence = 0.3364, Error Diff = 0.000070\n",
      "Iteration 125: KL Divergence = 0.3365, Error Diff = 0.000098\n",
      "Iteration 130: KL Divergence = 0.3361, Error Diff = 0.000455\n",
      "Iteration 135: KL Divergence = 0.3356, Error Diff = 0.000428\n",
      "Iteration 140: KL Divergence = 0.3355, Error Diff = 0.000176\n",
      "Iteration 145: KL Divergence = 0.3350, Error Diff = 0.000494\n",
      "Iteration 150: KL Divergence = 0.3350, Error Diff = 0.000007\n",
      "Iteration  150, KL divergence 0.3350, 50 iterations in 0.4364 sec\n",
      "Iteration 155: KL Divergence = 0.3344, Error Diff = 0.000606\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n",
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.6114, Relative Change = -0.4953%\n",
      "Iteration 9: KL Divergence = 1.3261, Relative Change = 17.7029%\n",
      "Iteration 12: KL Divergence = 0.7245, Relative Change = 45.3658%\n",
      "Iteration 15: KL Divergence = 0.4776, Relative Change = 34.0834%\n",
      "Iteration 18: KL Divergence = 0.4090, Relative Change = 14.3685%\n",
      "Iteration 21: KL Divergence = 0.3813, Relative Change = 6.7585%\n",
      "Iteration 24: KL Divergence = 0.3685, Relative Change = 3.3585%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.3511, Error Diff = 0.006054\n",
      "Iteration 15: KL Divergence = 0.3469, Error Diff = 0.004211\n",
      "Iteration 20: KL Divergence = 0.3447, Error Diff = 0.002250\n",
      "Iteration 25: KL Divergence = 0.3430, Error Diff = 0.001616\n",
      "Iteration 30: KL Divergence = 0.3422, Error Diff = 0.000803\n",
      "Iteration 35: KL Divergence = 0.3421, Error Diff = 0.000108\n",
      "Iteration 40: KL Divergence = 0.3415, Error Diff = 0.000604\n",
      "Iteration 45: KL Divergence = 0.3404, Error Diff = 0.001129\n",
      "Iteration 50: KL Divergence = 0.3394, Error Diff = 0.001040\n",
      "Iteration   50, KL divergence 0.3394, 50 iterations in 0.2357 sec\n",
      "Iteration 55: KL Divergence = 0.3391, Error Diff = 0.000279\n",
      "Iteration 60: KL Divergence = 0.3386, Error Diff = 0.000440\n",
      "Iteration 65: KL Divergence = 0.3384, Error Diff = 0.000222\n",
      "Iteration 70: KL Divergence = 0.3382, Error Diff = 0.000188\n",
      "Iteration 75: KL Divergence = 0.3380, Error Diff = 0.000276\n",
      "Iteration 80: KL Divergence = 0.3375, Error Diff = 0.000423\n",
      "Iteration 85: KL Divergence = 0.3377, Error Diff = 0.000144\n",
      "Iteration 90: KL Divergence = 0.3372, Error Diff = 0.000490\n",
      "Iteration 95: KL Divergence = 0.3366, Error Diff = 0.000555\n",
      "Iteration 100: KL Divergence = 0.3367, Error Diff = 0.000097\n",
      "Iteration  100, KL divergence 0.3367, 50 iterations in 0.2170 sec\n",
      "Iteration 105: KL Divergence = 0.3367, Error Diff = 0.000056\n",
      "Iteration 110: KL Divergence = 0.3365, Error Diff = 0.000126\n",
      "Iteration 115: KL Divergence = 0.3368, Error Diff = 0.000246\n",
      "Iteration 120: KL Divergence = 0.3364, Error Diff = 0.000389\n",
      "Iteration 125: KL Divergence = 0.3363, Error Diff = 0.000150\n",
      "Iteration 130: KL Divergence = 0.3358, Error Diff = 0.000435\n",
      "Iteration 135: KL Divergence = 0.3358, Error Diff = 0.000042\n",
      "Iteration 140: KL Divergence = 0.3355, Error Diff = 0.000253\n",
      "Iteration 145: KL Divergence = 0.3351, Error Diff = 0.000461\n",
      "Iteration 150: KL Divergence = 0.3347, Error Diff = 0.000396\n",
      "Iteration  150, KL divergence 0.3347, 50 iterations in 0.2151 sec\n",
      "Iteration 155: KL Divergence = 0.3342, Error Diff = 0.000509\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n",
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.00 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.6114, Relative Change = -0.2454%\n",
      "Iteration 9: KL Divergence = 1.3212, Relative Change = 18.0082%\n",
      "Iteration 12: KL Divergence = 0.7229, Relative Change = 45.2835%\n",
      "Iteration 15: KL Divergence = 0.4777, Relative Change = 33.9137%\n",
      "Iteration 18: KL Divergence = 0.4107, Relative Change = 14.0264%\n",
      "Iteration 21: KL Divergence = 0.3824, Relative Change = 6.9101%\n",
      "Iteration 24: KL Divergence = 0.3687, Relative Change = 3.5749%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.3514, Error Diff = 0.006720\n",
      "Iteration 15: KL Divergence = 0.3463, Error Diff = 0.005128\n",
      "Iteration 20: KL Divergence = 0.3445, Error Diff = 0.001785\n",
      "Iteration 25: KL Divergence = 0.3435, Error Diff = 0.000977\n",
      "Iteration 30: KL Divergence = 0.3428, Error Diff = 0.000781\n",
      "Iteration 35: KL Divergence = 0.3421, Error Diff = 0.000625\n",
      "Iteration 40: KL Divergence = 0.3415, Error Diff = 0.000657\n",
      "Iteration 45: KL Divergence = 0.3406, Error Diff = 0.000917\n",
      "Iteration 50: KL Divergence = 0.3395, Error Diff = 0.001069\n",
      "Iteration   50, KL divergence 0.3395, 50 iterations in 0.2320 sec\n",
      "Iteration 55: KL Divergence = 0.3393, Error Diff = 0.000159\n",
      "Iteration 60: KL Divergence = 0.3387, Error Diff = 0.000678\n",
      "Iteration 65: KL Divergence = 0.3384, Error Diff = 0.000281\n",
      "Iteration 70: KL Divergence = 0.3378, Error Diff = 0.000567\n",
      "Iteration 75: KL Divergence = 0.3380, Error Diff = 0.000163\n",
      "Iteration 80: KL Divergence = 0.3375, Error Diff = 0.000429\n",
      "Iteration 85: KL Divergence = 0.3376, Error Diff = 0.000041\n",
      "Iteration 90: KL Divergence = 0.3374, Error Diff = 0.000142\n",
      "Iteration 95: KL Divergence = 0.3370, Error Diff = 0.000430\n",
      "Iteration 100: KL Divergence = 0.3364, Error Diff = 0.000613\n",
      "Iteration  100, KL divergence 0.3364, 50 iterations in 0.2099 sec\n",
      "Iteration 105: KL Divergence = 0.3366, Error Diff = 0.000206\n",
      "Iteration 110: KL Divergence = 0.3365, Error Diff = 0.000111\n",
      "Iteration 115: KL Divergence = 0.3367, Error Diff = 0.000185\n",
      "Iteration 120: KL Divergence = 0.3363, Error Diff = 0.000372\n",
      "Iteration 125: KL Divergence = 0.3361, Error Diff = 0.000215\n",
      "Iteration 130: KL Divergence = 0.3360, Error Diff = 0.000107\n",
      "Iteration 135: KL Divergence = 0.3357, Error Diff = 0.000257\n",
      "Iteration 140: KL Divergence = 0.3354, Error Diff = 0.000361\n",
      "Iteration 145: KL Divergence = 0.3348, Error Diff = 0.000555\n",
      "Iteration 150: KL Divergence = 0.3347, Error Diff = 0.000081\n",
      "Iteration  150, KL divergence 0.3347, 50 iterations in 0.2140 sec\n",
      "Iteration 155: KL Divergence = 0.3342, Error Diff = 0.000504\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n",
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.7643, Relative Change = 0.0168%\n",
      "Iteration 9: KL Divergence = 1.5474, Relative Change = 12.2913%\n",
      "Iteration 12: KL Divergence = 0.8949, Relative Change = 42.1711%\n",
      "Iteration 15: KL Divergence = 0.7255, Relative Change = 18.9279%\n",
      "Iteration 18: KL Divergence = 0.6745, Relative Change = 7.0282%\n",
      "Iteration 21: KL Divergence = 0.6494, Relative Change = 3.7191%\n",
      "Iteration 24: KL Divergence = 0.6335, Relative Change = 2.4514%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.6066, Error Diff = 0.011820\n",
      "Iteration 15: KL Divergence = 0.5988, Error Diff = 0.007790\n",
      "Iteration 20: KL Divergence = 0.5938, Error Diff = 0.004953\n",
      "Iteration 25: KL Divergence = 0.5903, Error Diff = 0.003515\n",
      "Iteration 30: KL Divergence = 0.5883, Error Diff = 0.002016\n",
      "Iteration 35: KL Divergence = 0.5868, Error Diff = 0.001480\n",
      "Iteration 40: KL Divergence = 0.5849, Error Diff = 0.001960\n",
      "Iteration 45: KL Divergence = 0.5837, Error Diff = 0.001195\n",
      "Iteration 50: KL Divergence = 0.5825, Error Diff = 0.001135\n",
      "Iteration   50, KL divergence 0.5825, 50 iterations in 0.2301 sec\n",
      "Iteration 55: KL Divergence = 0.5813, Error Diff = 0.001251\n",
      "Iteration 60: KL Divergence = 0.5801, Error Diff = 0.001148\n",
      "Iteration 65: KL Divergence = 0.5790, Error Diff = 0.001168\n",
      "Iteration 70: KL Divergence = 0.5780, Error Diff = 0.000938\n",
      "Iteration 75: KL Divergence = 0.5775, Error Diff = 0.000558\n",
      "Iteration 80: KL Divergence = 0.5764, Error Diff = 0.001120\n",
      "Iteration 85: KL Divergence = 0.5761, Error Diff = 0.000291\n",
      "Iteration 90: KL Divergence = 0.5760, Error Diff = 0.000037\n",
      "Iteration 95: KL Divergence = 0.5760, Error Diff = 0.000064\n",
      "Iteration 100: KL Divergence = 0.5757, Error Diff = 0.000307\n",
      "Iteration  100, KL divergence 0.5757, 50 iterations in 0.1986 sec\n",
      "Iteration 105: KL Divergence = 0.5753, Error Diff = 0.000321\n",
      "Iteration 110: KL Divergence = 0.5752, Error Diff = 0.000096\n",
      "Iteration 115: KL Divergence = 0.5751, Error Diff = 0.000104\n",
      "Iteration 120: KL Divergence = 0.5752, Error Diff = 0.000027\n",
      "Iteration 125: KL Divergence = 0.5748, Error Diff = 0.000353\n",
      "Iteration 130: KL Divergence = 0.5747, Error Diff = 0.000090\n",
      "Iteration 135: KL Divergence = 0.5750, Error Diff = 0.000260\n",
      "Iteration 140: KL Divergence = 0.5749, Error Diff = 0.000124\n",
      "Iteration 145: KL Divergence = 0.5747, Error Diff = 0.000114\n",
      "Iteration 150: KL Divergence = 0.5747, Error Diff = 0.000007\n",
      "Iteration  150, KL divergence 0.5747, 50 iterations in 0.2012 sec\n",
      "Iteration 155: KL Divergence = 0.5734, Error Diff = 0.001333\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n",
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.00 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.7643, Relative Change = 0.0167%\n",
      "Iteration 9: KL Divergence = 1.5478, Relative Change = 12.2720%\n",
      "Iteration 12: KL Divergence = 0.8948, Relative Change = 42.1880%\n",
      "Iteration 15: KL Divergence = 0.7253, Relative Change = 18.9412%\n",
      "Iteration 18: KL Divergence = 0.6739, Relative Change = 7.0829%\n",
      "Iteration 21: KL Divergence = 0.6496, Relative Change = 3.6130%\n",
      "Iteration 24: KL Divergence = 0.6340, Relative Change = 2.3995%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.6085, Error Diff = 0.012043\n",
      "Iteration 15: KL Divergence = 0.6017, Error Diff = 0.006794\n",
      "Iteration 20: KL Divergence = 0.5969, Error Diff = 0.004798\n",
      "Iteration 25: KL Divergence = 0.5928, Error Diff = 0.004126\n",
      "Iteration 30: KL Divergence = 0.5908, Error Diff = 0.001995\n",
      "Iteration 35: KL Divergence = 0.5886, Error Diff = 0.002139\n",
      "Iteration 40: KL Divergence = 0.5859, Error Diff = 0.002711\n",
      "Iteration 45: KL Divergence = 0.5844, Error Diff = 0.001499\n",
      "Iteration 50: KL Divergence = 0.5827, Error Diff = 0.001712\n",
      "Iteration   50, KL divergence 0.5827, 50 iterations in 0.2191 sec\n",
      "Iteration 55: KL Divergence = 0.5817, Error Diff = 0.000985\n",
      "Iteration 60: KL Divergence = 0.5800, Error Diff = 0.001769\n",
      "Iteration 65: KL Divergence = 0.5792, Error Diff = 0.000732\n",
      "Iteration 70: KL Divergence = 0.5785, Error Diff = 0.000756\n",
      "Iteration 75: KL Divergence = 0.5777, Error Diff = 0.000744\n",
      "Iteration 80: KL Divergence = 0.5769, Error Diff = 0.000851\n",
      "Iteration 85: KL Divergence = 0.5767, Error Diff = 0.000140\n",
      "Iteration 90: KL Divergence = 0.5764, Error Diff = 0.000336\n",
      "Iteration 95: KL Divergence = 0.5759, Error Diff = 0.000534\n",
      "Iteration 100: KL Divergence = 0.5758, Error Diff = 0.000043\n",
      "Iteration  100, KL divergence 0.5758, 50 iterations in 0.2013 sec\n",
      "Iteration 105: KL Divergence = 0.5758, Error Diff = 0.000055\n",
      "Iteration 110: KL Divergence = 0.5754, Error Diff = 0.000407\n",
      "Iteration 115: KL Divergence = 0.5735, Error Diff = 0.001838\n",
      "Iteration 120: KL Divergence = 0.5730, Error Diff = 0.000498\n",
      "Iteration 125: KL Divergence = 0.5726, Error Diff = 0.000380\n",
      "Iteration 130: KL Divergence = 0.5722, Error Diff = 0.000408\n",
      "Iteration 135: KL Divergence = 0.5719, Error Diff = 0.000330\n",
      "Iteration 140: KL Divergence = 0.5715, Error Diff = 0.000359\n",
      "Iteration 145: KL Divergence = 0.5708, Error Diff = 0.000718\n",
      "Iteration 150: KL Divergence = 0.5706, Error Diff = 0.000217\n",
      "Iteration  150, KL divergence 0.5706, 50 iterations in 0.1990 sec\n",
      "Iteration 155: KL Divergence = 0.5704, Error Diff = 0.000164\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n",
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.7643, Relative Change = 0.0168%\n",
      "Iteration 9: KL Divergence = 1.5475, Relative Change = 12.2877%\n",
      "Iteration 12: KL Divergence = 0.8949, Relative Change = 42.1717%\n",
      "Iteration 15: KL Divergence = 0.7255, Relative Change = 18.9332%\n",
      "Iteration 18: KL Divergence = 0.6744, Relative Change = 7.0396%\n",
      "Iteration 21: KL Divergence = 0.6492, Relative Change = 3.7309%\n",
      "Iteration 24: KL Divergence = 0.6341, Relative Change = 2.3255%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.6072, Error Diff = 0.013742\n",
      "Iteration 15: KL Divergence = 0.5998, Error Diff = 0.007316\n",
      "Iteration 20: KL Divergence = 0.5947, Error Diff = 0.005130\n",
      "Iteration 25: KL Divergence = 0.5907, Error Diff = 0.003962\n",
      "Iteration 30: KL Divergence = 0.5889, Error Diff = 0.001873\n",
      "Iteration 35: KL Divergence = 0.5869, Error Diff = 0.001998\n",
      "Iteration 40: KL Divergence = 0.5852, Error Diff = 0.001712\n",
      "Iteration 45: KL Divergence = 0.5842, Error Diff = 0.000976\n",
      "Iteration 50: KL Divergence = 0.5828, Error Diff = 0.001421\n",
      "Iteration   50, KL divergence 0.5828, 50 iterations in 0.2282 sec\n",
      "Iteration 55: KL Divergence = 0.5814, Error Diff = 0.001389\n",
      "Iteration 60: KL Divergence = 0.5800, Error Diff = 0.001419\n",
      "Iteration 65: KL Divergence = 0.5792, Error Diff = 0.000750\n",
      "Iteration 70: KL Divergence = 0.5782, Error Diff = 0.001033\n",
      "Iteration 75: KL Divergence = 0.5775, Error Diff = 0.000661\n",
      "Iteration 80: KL Divergence = 0.5769, Error Diff = 0.000578\n",
      "Iteration 85: KL Divergence = 0.5764, Error Diff = 0.000500\n",
      "Iteration 90: KL Divergence = 0.5764, Error Diff = 0.000078\n",
      "Iteration 95: KL Divergence = 0.5760, Error Diff = 0.000395\n",
      "Iteration 100: KL Divergence = 0.5756, Error Diff = 0.000333\n",
      "Iteration  100, KL divergence 0.5756, 50 iterations in 0.2151 sec\n",
      "Iteration 105: KL Divergence = 0.5755, Error Diff = 0.000115\n",
      "Iteration 110: KL Divergence = 0.5754, Error Diff = 0.000153\n",
      "Iteration 115: KL Divergence = 0.5751, Error Diff = 0.000280\n",
      "Iteration 120: KL Divergence = 0.5740, Error Diff = 0.001105\n",
      "Iteration 125: KL Divergence = 0.5731, Error Diff = 0.000924\n",
      "Iteration 130: KL Divergence = 0.5721, Error Diff = 0.000905\n",
      "Iteration 135: KL Divergence = 0.5715, Error Diff = 0.000640\n",
      "Iteration 140: KL Divergence = 0.5710, Error Diff = 0.000551\n",
      "Iteration 145: KL Divergence = 0.5708, Error Diff = 0.000143\n",
      "Iteration 150: KL Divergence = 0.5703, Error Diff = 0.000542\n",
      "Iteration  150, KL divergence 0.5703, 50 iterations in 0.2083 sec\n",
      "Iteration 155: KL Divergence = 0.5704, Error Diff = 0.000125\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n",
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.02 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.7982, Relative Change = 0.0908%\n",
      "Iteration 9: KL Divergence = 0.9825, Relative Change = 45.3643%\n",
      "Iteration 12: KL Divergence = 0.4186, Relative Change = 57.3934%\n",
      "Iteration 15: KL Divergence = 0.3017, Relative Change = 27.9350%\n",
      "Iteration 18: KL Divergence = 0.2766, Relative Change = 8.3084%\n",
      "Iteration 21: KL Divergence = 0.2558, Relative Change = 7.5301%\n",
      "Iteration 24: KL Divergence = 0.2440, Relative Change = 4.6115%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.2290, Error Diff = 0.005456\n",
      "Iteration 15: KL Divergence = 0.2266, Error Diff = 0.002418\n",
      "Iteration 20: KL Divergence = 0.2253, Error Diff = 0.001321\n",
      "Iteration 25: KL Divergence = 0.2239, Error Diff = 0.001354\n",
      "Iteration 30: KL Divergence = 0.2233, Error Diff = 0.000592\n",
      "Iteration 35: KL Divergence = 0.2227, Error Diff = 0.000617\n",
      "Iteration 40: KL Divergence = 0.2221, Error Diff = 0.000652\n",
      "Iteration 45: KL Divergence = 0.2212, Error Diff = 0.000908\n",
      "Iteration 50: KL Divergence = 0.2208, Error Diff = 0.000325\n",
      "Iteration   50, KL divergence 0.2208, 50 iterations in 0.1983 sec\n",
      "Iteration 55: KL Divergence = 0.2199, Error Diff = 0.000917\n",
      "Iteration 60: KL Divergence = 0.2187, Error Diff = 0.001208\n",
      "Iteration 65: KL Divergence = 0.2171, Error Diff = 0.001595\n",
      "Iteration 70: KL Divergence = 0.2166, Error Diff = 0.000485\n",
      "Iteration 75: KL Divergence = 0.2156, Error Diff = 0.001040\n",
      "Iteration 80: KL Divergence = 0.2144, Error Diff = 0.001166\n",
      "Iteration 85: KL Divergence = 0.2120, Error Diff = 0.002450\n",
      "Iteration 90: KL Divergence = 0.2105, Error Diff = 0.001504\n",
      "Iteration 95: KL Divergence = 0.2098, Error Diff = 0.000703\n",
      "Iteration 100: KL Divergence = 0.2092, Error Diff = 0.000555\n",
      "Iteration  100, KL divergence 0.2092, 50 iterations in 0.1892 sec\n",
      "Iteration 105: KL Divergence = 0.2088, Error Diff = 0.000409\n",
      "Iteration 110: KL Divergence = 0.2085, Error Diff = 0.000321\n",
      "Iteration 115: KL Divergence = 0.2081, Error Diff = 0.000405\n",
      "Iteration 120: KL Divergence = 0.2077, Error Diff = 0.000328\n",
      "Iteration 125: KL Divergence = 0.2076, Error Diff = 0.000110\n",
      "Iteration 130: KL Divergence = 0.2074, Error Diff = 0.000266\n",
      "Iteration 135: KL Divergence = 0.2075, Error Diff = 0.000160\n",
      "Iteration 140: KL Divergence = 0.2073, Error Diff = 0.000253\n",
      "Iteration 145: KL Divergence = 0.2071, Error Diff = 0.000161\n",
      "Iteration 150: KL Divergence = 0.2071, Error Diff = 0.000047\n",
      "Iteration  150, KL divergence 0.2071, 50 iterations in 0.2025 sec\n",
      "Iteration 155: KL Divergence = 0.2068, Error Diff = 0.000250\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n",
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.02 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.7982, Relative Change = 0.0939%\n",
      "Iteration 9: KL Divergence = 0.9820, Relative Change = 45.3933%\n",
      "Iteration 12: KL Divergence = 0.4186, Relative Change = 57.3706%\n",
      "Iteration 15: KL Divergence = 0.3015, Relative Change = 27.9669%\n",
      "Iteration 18: KL Divergence = 0.2765, Relative Change = 8.2917%\n",
      "Iteration 21: KL Divergence = 0.2553, Relative Change = 7.6609%\n",
      "Iteration 24: KL Divergence = 0.2439, Relative Change = 4.4789%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.2289, Error Diff = 0.005269\n",
      "Iteration 15: KL Divergence = 0.2266, Error Diff = 0.002374\n",
      "Iteration 20: KL Divergence = 0.2256, Error Diff = 0.001007\n",
      "Iteration 25: KL Divergence = 0.2241, Error Diff = 0.001441\n",
      "Iteration 30: KL Divergence = 0.2233, Error Diff = 0.000822\n",
      "Iteration 35: KL Divergence = 0.2230, Error Diff = 0.000316\n",
      "Iteration 40: KL Divergence = 0.2222, Error Diff = 0.000802\n",
      "Iteration 45: KL Divergence = 0.2216, Error Diff = 0.000594\n",
      "Iteration 50: KL Divergence = 0.2210, Error Diff = 0.000541\n",
      "Iteration   50, KL divergence 0.2210, 50 iterations in 0.2875 sec\n",
      "Iteration 55: KL Divergence = 0.2204, Error Diff = 0.000615\n",
      "Iteration 60: KL Divergence = 0.2193, Error Diff = 0.001082\n",
      "Iteration 65: KL Divergence = 0.2185, Error Diff = 0.000826\n",
      "Iteration 70: KL Divergence = 0.2174, Error Diff = 0.001067\n",
      "Iteration 75: KL Divergence = 0.2166, Error Diff = 0.000886\n",
      "Iteration 80: KL Divergence = 0.2157, Error Diff = 0.000817\n",
      "Iteration 85: KL Divergence = 0.2150, Error Diff = 0.000716\n",
      "Iteration 90: KL Divergence = 0.2139, Error Diff = 0.001167\n",
      "Iteration 95: KL Divergence = 0.2126, Error Diff = 0.001291\n",
      "Iteration 100: KL Divergence = 0.2101, Error Diff = 0.002438\n",
      "Iteration  100, KL divergence 0.2101, 50 iterations in 0.2618 sec\n",
      "Iteration 105: KL Divergence = 0.2094, Error Diff = 0.000777\n",
      "Iteration 110: KL Divergence = 0.2091, Error Diff = 0.000243\n",
      "Iteration 115: KL Divergence = 0.2082, Error Diff = 0.000865\n",
      "Iteration 120: KL Divergence = 0.2079, Error Diff = 0.000380\n",
      "Iteration 125: KL Divergence = 0.2075, Error Diff = 0.000336\n",
      "Iteration 130: KL Divergence = 0.2074, Error Diff = 0.000167\n",
      "Iteration 135: KL Divergence = 0.2076, Error Diff = 0.000239\n",
      "Iteration 140: KL Divergence = 0.2072, Error Diff = 0.000402\n",
      "Iteration 145: KL Divergence = 0.2072, Error Diff = 0.000014\n",
      "Iteration 150: KL Divergence = 0.2071, Error Diff = 0.000108\n",
      "Iteration  150, KL divergence 0.2071, 50 iterations in 0.1945 sec\n",
      "Iteration 155: KL Divergence = 0.2068, Error Diff = 0.000317\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n",
      "===> Finding 90 nearest neighbors using exact search using euclidean distance...\n",
      "   --> Time elapsed: 0.02 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 6: KL Divergence = 1.7982, Relative Change = 0.0714%\n",
      "Iteration 9: KL Divergence = 0.9823, Relative Change = 45.3749%\n",
      "Iteration 12: KL Divergence = 0.4185, Relative Change = 57.3952%\n",
      "Iteration 15: KL Divergence = 0.3017, Relative Change = 27.9191%\n",
      "Iteration 18: KL Divergence = 0.2764, Relative Change = 8.3595%\n",
      "Iteration 21: KL Divergence = 0.2555, Relative Change = 7.5804%\n",
      "Iteration 24: KL Divergence = 0.2439, Relative Change = 4.5243%\n",
      "Relative change has consistently decreased. Stopping Early Exaggeration.\n",
      "EE Iteration stopped at 24\n",
      "===> Running optimization with exaggeration=1.00, lr=200.00 for 1000 iterations...\n",
      "Iteration 10: KL Divergence = 0.2289, Error Diff = 0.005404\n",
      "Iteration 15: KL Divergence = 0.2265, Error Diff = 0.002433\n",
      "Iteration 20: KL Divergence = 0.2256, Error Diff = 0.000914\n",
      "Iteration 25: KL Divergence = 0.2240, Error Diff = 0.001526\n",
      "Iteration 30: KL Divergence = 0.2235, Error Diff = 0.000559\n",
      "Iteration 35: KL Divergence = 0.2230, Error Diff = 0.000495\n",
      "Iteration 40: KL Divergence = 0.2222, Error Diff = 0.000756\n",
      "Iteration 45: KL Divergence = 0.2216, Error Diff = 0.000620\n",
      "Iteration 50: KL Divergence = 0.2211, Error Diff = 0.000502\n",
      "Iteration   50, KL divergence 0.2211, 50 iterations in 0.1801 sec\n",
      "Iteration 55: KL Divergence = 0.2205, Error Diff = 0.000630\n",
      "Iteration 60: KL Divergence = 0.2194, Error Diff = 0.001050\n",
      "Iteration 65: KL Divergence = 0.2184, Error Diff = 0.001054\n",
      "Iteration 70: KL Divergence = 0.2169, Error Diff = 0.001490\n",
      "Iteration 75: KL Divergence = 0.2165, Error Diff = 0.000357\n",
      "Iteration 80: KL Divergence = 0.2158, Error Diff = 0.000688\n",
      "Iteration 85: KL Divergence = 0.2148, Error Diff = 0.001079\n",
      "Iteration 90: KL Divergence = 0.2132, Error Diff = 0.001567\n",
      "Iteration 95: KL Divergence = 0.2114, Error Diff = 0.001790\n",
      "Iteration 100: KL Divergence = 0.2096, Error Diff = 0.001786\n",
      "Iteration  100, KL divergence 0.2096, 50 iterations in 0.1903 sec\n",
      "Iteration 105: KL Divergence = 0.2093, Error Diff = 0.000329\n",
      "Iteration 110: KL Divergence = 0.2089, Error Diff = 0.000403\n",
      "Iteration 115: KL Divergence = 0.2083, Error Diff = 0.000605\n",
      "Iteration 120: KL Divergence = 0.2079, Error Diff = 0.000392\n",
      "Iteration 125: KL Divergence = 0.2077, Error Diff = 0.000193\n",
      "Iteration 130: KL Divergence = 0.2074, Error Diff = 0.000287\n",
      "Iteration 135: KL Divergence = 0.2074, Error Diff = 0.000028\n",
      "Iteration 140: KL Divergence = 0.2071, Error Diff = 0.000299\n",
      "Iteration 145: KL Divergence = 0.2069, Error Diff = 0.000202\n",
      "Iteration 150: KL Divergence = 0.2073, Error Diff = 0.000370\n",
      "Iteration  150, KL divergence 0.2073, 50 iterations in 0.3221 sec\n",
      "Iteration 155: KL Divergence = 0.2068, Error Diff = 0.000477\n",
      "KL divergence change is below threshold. Stopping optimization.\n",
      "Run iteration stopped at 155\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = run_tsne_experiments(all_data, seeds=seeds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_opt, labels_opt, monitor_data = embeddings_dict[((0, \"opt\"), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergences (Iteration, Value):\n",
      "Iteration 3: 1.6079962199176752\n",
      "Iteration 6: 1.6113907656585376\n",
      "Iteration 9: 1.322432435558424\n",
      "Iteration 12: 0.7190336896022629\n",
      "Iteration 15: 0.473380544627636\n",
      "Iteration 18: 0.40246576535513157\n",
      "Iteration 21: 0.3804921785248423\n",
      "Iteration 24: 0.3672265175758662\n",
      "Iteration 29: 0.3570147693079111\n",
      "Iteration 34: 0.3506412262075367\n",
      "Iteration 39: 0.3462271269759558\n",
      "Iteration 44: 0.3441015183438685\n",
      "Iteration 49: 0.3430746331324306\n",
      "Iteration 54: 0.342046282878008\n",
      "Iteration 59: 0.3418538223090728\n",
      "Iteration 64: 0.3411755483566674\n",
      "Iteration 69: 0.34025732153077826\n",
      "Iteration 74: 0.3394142574024608\n",
      "Iteration 79: 0.3391280773745944\n",
      "Iteration 84: 0.33854963257214266\n",
      "Iteration 89: 0.33829287734261104\n",
      "Iteration 94: 0.33771560026788805\n",
      "Iteration 99: 0.33771744039818063\n",
      "Iteration 104: 0.3373654077081323\n",
      "Iteration 109: 0.3373460362891594\n",
      "Iteration 114: 0.33767755257701637\n",
      "Iteration 119: 0.3373090252516384\n",
      "Iteration 124: 0.33664722472556097\n",
      "Iteration 129: 0.33647796898469284\n",
      "Iteration 134: 0.33655337504974536\n",
      "Iteration 139: 0.3364909398165761\n",
      "Iteration 144: 0.3364205027933762\n",
      "Iteration 149: 0.33651815715734745\n",
      "Iteration 154: 0.33606322227737095\n",
      "Iteration 159: 0.3356347497398202\n",
      "Iteration 164: 0.3354588817426061\n",
      "Iteration 169: 0.3349645674668533\n",
      "Iteration 174: 0.3349714636646164\n",
      "Iteration 179: 0.3343654101555229\n",
      "\n",
      "KL Divergence Relative Changes (Iteration, Value):\n",
      "Iteration 6: -0.2111040871126046\n",
      "Iteration 9: 17.932231973665495\n",
      "Iteration 12: 45.62794512079279\n",
      "Iteration 15: 34.16434424797413\n",
      "Iteration 18: 14.980501433215098\n",
      "Iteration 21: 5.459740609465257\n",
      "Iteration 24: 3.486447737350786\n"
     ]
    }
   ],
   "source": [
    "kld_opt = monitor_data[\"kl_divergences\"]    \n",
    "rel_changes = monitor_data[\"rel_changes\"]     \n",
    "\n",
    "# Print the values:\n",
    "print(\"KL Divergences (Iteration, Value):\")\n",
    "for iteration, kld_value in kld_opt:\n",
    "    print(f\"Iteration {iteration}: {kld_value}\")\n",
    "\n",
    "print(\"\\nKL Divergence Relative Changes (Iteration, Value):\")\n",
    "for iteration, rel_change in rel_changes:\n",
    "    print(f\"Iteration {iteration}: {rel_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_embedding_grid_experiments(embeddings_dict, seeds, [0,1,2], cmap=\"tab20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wissrech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
