{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Automated Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 00:03:10.445639: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "from openTSNE import TSNE\n",
    "import openTSNE\n",
    "import time\n",
    "import json\n",
    "from math import ceil\n",
    "\n",
    "# for local imports  \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os \n",
    "\n",
    "script_dir = Path.cwd().parent / \"scripts\"\n",
    "sys.path.append(str(script_dir))\n",
    "\n",
    "figures_dir = Path.cwd().parent / \"figures\"\n",
    "sys.path.append(str(figures_dir))\n",
    "\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "sys.path.append(str(results_dir))\n",
    "\n",
    "import datasets\n",
    "import quality_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KLD and KLDRC Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import callbacks\n",
    "class KLDRCMonitorNoOpt(callbacks.Callback):\n",
    "    def __init__(self, record_every=5):\n",
    "     \n",
    "        self.record_every = record_every  # Equivalent to `auto_iter_pollrate_ee = 3`\n",
    "        self.kl_divergences = []\n",
    "\n",
    "    def __call__(self, iteration, error, embedding):\n",
    "        \"\"\"\n",
    "        Monitors KL divergence. \n",
    "        \"\"\"\n",
    "        # Only check every `record_every` iterations\n",
    "        if iteration % self.record_every == 0:\n",
    "            self.kl_divergences.append((iteration, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_ee = 15\n",
    "switch_buffer = 2 \n",
    "\n",
    "class KLDRCMonitorEE(callbacks.Callback):\n",
    "    def __init__(self, record_every=3):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            record_every (int): Check KL divergence every this many iterations.\n",
    "            buffer_ee (int): Minimum iterations before monitoring KL divergence.\n",
    "            switch_buffer (int): Extra iterations to confirm EE phase exit.\n",
    "        \"\"\"\n",
    "        self.record_every = record_every  # Equivalent to `auto_iter_pollrate_ee = 3`\n",
    "        self.kl_divergences = []\n",
    "        self.last_error = None\n",
    "        self.last_rel_change = None\n",
    "        self.switch_buffer_count = switch_buffer  # Tracks remaining iterations before exiting EE\n",
    "\n",
    "    def __call__(self, iteration, error, embedding):\n",
    "        \"\"\"\n",
    "        Monitors KL divergence and determines when to stop Early Exaggeration.\n",
    "        Returns True if EE should stop.\n",
    "        \"\"\"\n",
    "        # Only check every `record_every` iterations\n",
    "        if iteration % self.record_every == 0:\n",
    "            self.kl_divergences.append((iteration, error))\n",
    "\n",
    "            if self.last_error is not None:\n",
    "                # Compute relative change: (prev_error - current_error) / prev_error\n",
    "                rel_change = 100 * (self.last_error - error) / self.last_error  \n",
    "\n",
    "                print(f\"Iteration {iteration}: KL Divergence = {error:.4f}, Relative Change = {rel_change:.4f}%\")\n",
    "\n",
    "                # Start checking only after `buffer_ee` iterations\n",
    "                if iteration > buffer_ee:\n",
    "                    if self.last_rel_change is not None and rel_change < self.last_rel_change:\n",
    "                        # If relative change decreases, start the switch buffer countdown\n",
    "                        if self.switch_buffer_count < 1:\n",
    "                            print(\"Relative change has consistently decreased. Stopping Early Exaggeration.\")\n",
    "                            print(f\"EE Iteration stopped at {iteration}\")\n",
    "                            # Signal to stop EE phase, we return iteration as well \n",
    "                            return True, iteration  \n",
    "                        self.switch_buffer_count -= 1\n",
    "                    else:\n",
    "                        # Reset switch buffer if relative change increases again\n",
    "                        self.switch_buffer_count = switch_buffer\n",
    "\n",
    "                self.last_rel_change = rel_change\n",
    "\n",
    "            # Update last error for the next iteration\n",
    "            self.last_error = error\n",
    "\n",
    "        return False  # Continue EE phase if conditions are not met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_run = 150 \n",
    "auto_iter_end = 100 \n",
    "\n",
    "class KLDRCMonitorRun(callbacks.Callback):\n",
    "    def __init__(self, record_every=5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            record_every (int): Check KL divergence every this many iterations.\n",
    "            buffer_run (int): Minimum iterations after EE before monitoring for stopping.\n",
    "            auto_iter_end (float): Threshold for stopping, lower values stop earlier.\n",
    "        \"\"\"\n",
    "        self.record_every = record_every  # Equivalent to `auto_iter_pollrate_run = 5`\n",
    "        # self.buffer_run = buffer_run  # Equivalent to `auto_iter_buffer_run = 15`\n",
    "        # self.auto_iter_end = auto_iter_end  # Used for stopping condition\n",
    "\n",
    "        self.kl_divergences = []\n",
    "        self.last_error = None\n",
    "\n",
    "    def __call__(self, iteration, error, embedding):\n",
    "        \"\"\"\n",
    "        Monitors KL divergence and determines when to stop the full t-SNE run.\n",
    "        Returns True if the run should stop.\n",
    "        \"\"\"\n",
    "        # Only check KL divergence every `record_every` iterations\n",
    "        if iteration % self.record_every == 0:\n",
    "            self.kl_divergences.append((iteration, error))\n",
    "\n",
    "            if self.last_error is not None:\n",
    "                # Compute absolute error difference\n",
    "                error_diff = abs(self.last_error - error)\n",
    "\n",
    "                print(f\"Iteration {iteration}: KL Divergence = {error:.4f}, Error Diff = {error_diff:.6f}\")\n",
    "\n",
    "                # Start monitoring only after `buffer_run` iterations have passed\n",
    "                if iteration > buffer_run:\n",
    "                    # Stopping condition from C++: abs(error_diff)/pollrate < error/auto_iter_end\n",
    "                    if (error_diff / self.record_every) < (error / auto_iter_end):\n",
    "                        print(\"KL divergence change is below threshold. Stopping optimization.\")\n",
    "                        print(f\"Run iteration stopped at {iteration}\")\n",
    "                        return True, iteration  # Signal to stop t-SNE run\n",
    "\n",
    "            # Update last error\n",
    "            self.last_error = error\n",
    "\n",
    "        return False  # Continue t-SNE run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optSNE(data, labels, random_state):\n",
    "    \"\"\"\n",
    "    Runs the optimized t-SNE (i.e. with automated stopping) on a given dataset.\n",
    "    This version accepts data and labels as inputs (instead of using a global DataFrame).\n",
    "    \n",
    "    Returns:\n",
    "      embedding: the final embedding.\n",
    "      final_kld: final KL divergence value.\n",
    "      labels: the labels (unchanged).\n",
    "      kld_monitor: the monitor object containing the KL divergence history.\n",
    "    \"\"\"\n",
    "    # In this version we use the entire dataset.\n",
    "    features = data  # assuming data is already a numpy array\n",
    "    \n",
    "    initiali = openTSNE.initialization.pca(features, random_state=random_state)\n",
    "    \n",
    "    # using default perplexity\n",
    "    affinities = openTSNE.affinity.PerplexityBasedNN(\n",
    "        features,\n",
    "        perplexity=30, \n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    embedding_obj = openTSNE.TSNEEmbedding(\n",
    "        initiali, \n",
    "        affinities, \n",
    "        random_state=random_state,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # --- Early Exaggeration phase ---\n",
    "    kld_monitor_EE = KLDRCMonitorEE(record_every=3)\n",
    "    try:\n",
    "        embedding_obj = embedding_obj.optimize(\n",
    "            callbacks=kld_monitor_EE, \n",
    "            callbacks_every_iters=3, \n",
    "            verbose=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Early Exaggeration phase stopped early:\", e)\n",
    "\n",
    "    \n",
    "    # --- Embedding phase ---\n",
    "    kld_tracker_embed = KLDRCMonitorRun(record_every=5)\n",
    "    try:\n",
    "        embedding_obj = embedding_obj.optimize(\n",
    "            callbacks=kld_tracker_embed, \n",
    "            callbacks_every_iters=5, \n",
    "            verbose=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Embedding phase stopped early:\", e)\n",
    "        \n",
    "    # Return the embedding, labels, and the monitor (so its full history is available)\n",
    "    # TODO: we need to return both trackers \n",
    "    return embedding_obj, labels, kld_tracker_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne_experiments(datasets, seeds, default_EE=250, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs both normal t-SNE and optimized t-SNE (opt-SNE) on each dataset,\n",
    "    each with multiple seeds.\n",
    "    \n",
    "    Parameters:\n",
    "      datasets: list of tuples (data, labels)\n",
    "      seeds: list of seeds (e.g., 3 seeds)\n",
    "      default_EE: fixed early exaggeration iterations for normal t-SNE\n",
    "      verbose: verbosity flag\n",
    "      \n",
    "    Returns:\n",
    "      embeddings: dict with keys ((dataset_index, setting), seed) mapping to \n",
    "                  (embedding, labels, kld_history)\n",
    "      timings: list of dicts with timing info.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    timings = []\n",
    "    \n",
    "    for d_idx, dataset in enumerate(datasets):\n",
    "        data, labels = dataset\n",
    "        if hasattr(data, \"values\"):\n",
    "            data = data.values.astype(float)\n",
    "\n",
    "        for seed in seeds:\n",
    "            # --- Run normal t-SNE with default settings ---\n",
    "            # TODO: use real kld monitor\n",
    "            kld_monitor_norm = KLDMonitor(default_EE)  # using your monitor class\n",
    "            tsne_norm = TSNE(early_exaggeration_iter=default_EE, \n",
    "                             n_iter=750 - default_EE, \n",
    "                             n_jobs=-1,\n",
    "                             callbacks=kld_monitor_norm, \n",
    "                             callbacks_every_iters=5,\n",
    "                             random_state=seed, \n",
    "                             verbose=verbose)\n",
    "            t0 = time.time()\n",
    "            embedding_norm = tsne_norm.fit(data)\n",
    "            elapsed_normal = time.time() - t0\n",
    "            kld_norm = kld_monitor_norm.kl_divergences\n",
    "            embeddings[((d_idx, \"normal\"), seed)] = (embedding_norm, labels, kld_norm)\n",
    "            timings.append({\n",
    "                \"dataset_index\": d_idx, \n",
    "                \"setting\": \"normal\", \n",
    "                \"seed\": seed, \n",
    "                \"time_taken_seconds\": elapsed_normal\n",
    "            })\n",
    "            \n",
    "            # --- Run optimized t-SNE (opt-SNE) ---\n",
    "            t0 = time.time()\n",
    "            embedding_opt, labels_opt, kld_monitor_opt = run_optSNE(\n",
    "                data, labels, random_state=seed)\n",
    "            elapsed_opt = time.time() - t0\n",
    "            kld_opt = kld_monitor_opt.kl_divergences\n",
    "            embeddings[((d_idx, \"opt\"), seed)] = (embedding_opt, labels_opt, kld_opt)\n",
    "            timings.append({\n",
    "                \"dataset_index\": d_idx, \n",
    "                \"setting\": \"opt\", \n",
    "                \"seed\": seed, \n",
    "                \"time_taken_seconds\": elapsed_opt\n",
    "            })\n",
    "            \n",
    "    # Optionally, save timings to a file.\n",
    "    save_path = os.path.join(results_dir, \"opt-SNE_times.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(timings, f, indent=4)\n",
    "        \n",
    "    return embeddings, timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne_with_callbacks_and_timing(datasets, seeds, verbose=False, filename=\"ee_length_times.json\"):\n",
    "    \"\"\"Returns embedding dictionary with keys given by perplexity and dataset used to generate the embedding.\n",
    "    \n",
    "    Also tracks and saves the time taken for each embedding run to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    - datasets: list of tuples (data, labels)\n",
    "    - seeds: list of seeds for t-SNE (should be 3 )\n",
    "    - verbose (bool): If True, prints additional TSNE information.\n",
    "    - save_times_path (str): File path to automatically save the time tracking data.\n",
    "    \n",
    "    Returns:\n",
    "    - embedding_dict: dictionary with keys (perp, dataset_index) mapping to (embedding, labels, kld_values)\n",
    "    \"\"\"\n",
    "    embedding_dict = {}\n",
    "    timings = []  # List to store timing information for each embedding\n",
    "\n",
    "    save_times_path = os.path.join(results_dir, filename)\n",
    "\n",
    "    for ee_iter in tqdm(ee_lengths):\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            data, labels = dataset\n",
    "            # Convert to np.array if data is a DataFrame\n",
    "            if hasattr(data, \"values\"):\n",
    "                data = data.values.astype(float)\n",
    "\n",
    "            kld_monitor = KLDMonitor(ee_iter)\n",
    "            \n",
    "            tsne = TSNE(early_exaggeration_iter=ee_iter, n_iter=750 - ee_iter, n_jobs=-1,\n",
    "                        callbacks=kld_monitor, callbacks_every_iters=5,\n",
    "                        random_state=42, verbose=verbose)\n",
    "            # print(f\"running {ee_iter} EE iteration and {750-ee_iter} normal ones\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            embedding = tsne.fit(data)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            kld_values = kld_monitor.kl_divergences\n",
    "            embedding_dict[(ee_iter, i)] = (embedding, labels, kld_values)\n",
    "            \n",
    "            # Record timing data for this embedding\n",
    "            timings.append({\n",
    "                \"ee_length\": ee_iter,\n",
    "                \"dataset_index\": i,\n",
    "                \"time_taken_seconds\": elapsed_time\n",
    "            })\n",
    "\n",
    "    # Automatically save the timing information to the specified JSON file.\n",
    "    with open(save_times_path, \"w\") as f:\n",
    "        json.dump(timings, f, indent=4)\n",
    "    \n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne_with_seeds(dataset, ee_lengths, seeds, verbose=False):\n",
    "    \"\"\"Returns embedding dictionary with keys given by perplexity and dataset used to generate the embedding.\n",
    "    \n",
    "    Also tracks and saves the time taken for each embedding run to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: one dataset, tuple (data, labels)\n",
    "    - ee_lengths: list of ee lengths for t-SNE\n",
    "    - seeds: a list of seeds \n",
    "    - verbose (bool): If True, prints additional TSNE information.\n",
    "    \n",
    "    Returns:\n",
    "    - embedding_dict: dictionary with keys (perp, dataset_index) mapping to (embedding, labels, kld_values)\n",
    "    \"\"\"\n",
    "    embedding_dict = {}\n",
    "\n",
    "    for ee_iter in tqdm(ee_lengths):\n",
    "        for seed in seeds:\n",
    "            data, labels = dataset\n",
    "            # Convert to np.array if data is a DataFrame\n",
    "            if hasattr(data, \"values\"):\n",
    "                data = data.values.astype(float)\n",
    "\n",
    "            kld_monitor = KLDMonitor(ee_iter)\n",
    "            \n",
    "            tsne = TSNE(early_exaggeration_iter=ee_iter, n_iter=750 - ee_iter, n_jobs=-1,\n",
    "                        callbacks=kld_monitor, callbacks_every_iters=5,\n",
    "                        random_state=seed, verbose=verbose)\n",
    "            # print(f\"running {ee_iter} EE iteration and {750-ee_iter} normal ones\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            embedding = tsne.fit(data)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            kld_values = kld_monitor.kl_divergences\n",
    "            embedding_dict[(ee_iter, seed)] = (embedding, labels, kld_values)\n",
    "            \n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Grid Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding_grid(embeddings, row_parameter, column_parameter, row_string=\"\", column_string=\"\", cmap=\"tab20\", file_start=\"ee_length\", emph_first_row=True):\n",
    "    \"\"\"\n",
    "    Plots a grid of t-SNE embeddings with a common column label.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (dict): Keys are (col_value, row_value) tuples, and values are (embedding_array, labels, kld_values).\n",
    "    - row_parameter (list): Values for rows (e.g., datasets [0, 1, 2, 3]).\n",
    "    - column_parameter (list): Values for columns (e.g., number of iterations, perplexity, etc.).\n",
    "    - row_string (str): Label for the row axis.\n",
    "    - column_string (str): Common label for the column axis (displayed once over the grid).\n",
    "    - cmap: \"tab20\" by default \n",
    "    - file_start: e.g. \"eta\"\n",
    "    \"\"\"\n",
    "    num_rows = len(row_parameter)\n",
    "    num_cols = len(column_parameter)\n",
    "    \n",
    "    subplot_size = 3  \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, \n",
    "                             figsize=(12, subplot_size * num_rows), # (num_cols * subplot_size, num_rows * subplot_size), \n",
    "                             squeeze=False)\n",
    "    \n",
    "    # Add a common column label above the grid \n",
    "    fig.suptitle(column_string, fontsize=12)\n",
    "\n",
    "    # Plot embeddings in each subplot\n",
    "    for row_idx, row_value in enumerate(row_parameter):\n",
    "        for col_idx, col_value in enumerate(column_parameter):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            \n",
    "            # Unpack; kld_values is ignored here.\n",
    "            embedding, labels, _ = embeddings[(col_value, row_value)]\n",
    "\n",
    "            # plot scatterplot \n",
    "            ax.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap=cmap, s=7 if row_idx==0 and emph_first_row else 2, alpha=1.0 if row_idx==0 and emph_first_row else 0.6)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # Remove axis outlines\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            \n",
    "            # For the top row, display just the column value as the individual title\n",
    "            if row_idx == 0:\n",
    "                ax.set_title(f\"{col_value}\", fontsize=12, pad=10)\n",
    "            \n",
    "            # Optionally, add row labels on the left-most column\n",
    "            if col_idx == 0 and row_string:\n",
    "                ax.set_ylabel(f\"{row_string}: {row_value}\", fontsize=12, labelpad=10)\n",
    "    \n",
    "    # Adjust layout to make room for the common column label\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2, top=0.93)\n",
    "\n",
    "    # Save the figure to the specific folder\n",
    "    save_path = os.path.join(figures_dir, f\"{file_start}_embedding_grid_{cmap}.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Quality Measures (without KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quality_measures(quality_results, dataset_names, plot_name=\"ee_length\", x_axis_label=\"EE length\", x_axis_entries=None):\n",
    "    \"\"\"\n",
    "    Plots three side-by-side plots for the three embedding quality measures.\n",
    "\n",
    "    Parameters:\n",
    "        quality_results (dict): Maps x-axis entries to dataset indices with tuples (mnn, mnn_global, rho).\n",
    "        dataset_names (dict): Maps dataset indices to dataset names.\n",
    "        plot_name (str): For saving the file, e.g. \"perp\", \"n_iter\" etc.\n",
    "        x_axis_label (str): Label for the x-axis.\n",
    "        x_axis_entries (list): Custom x-axis entries; defaults to sorted keys of quality_results (this should be okay).\n",
    "    \"\"\"\n",
    "    if x_axis_entries is None:\n",
    "        x_axis_entries = list(quality_results.keys())\n",
    "\n",
    "    dataset_indices = sorted(next(iter(quality_results.values())).keys())  # Extract dataset indices\n",
    "    dataset_data = {d: {'mnn': [], 'mnn_global': [], 'rho': []} for d in dataset_indices}\n",
    "\n",
    "    # Extract data\n",
    "    for x in x_axis_entries:\n",
    "        for d in dataset_indices:\n",
    "            mnn, mnn_global, rho = quality_results[x][d]\n",
    "            dataset_data[d]['mnn'].append(mnn)\n",
    "            dataset_data[d]['mnn_global'].append(mnn_global)\n",
    "            dataset_data[d]['rho'].append(rho)\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    titles = [\"KNN\", \"KNC\", \"CPD\"]\n",
    "\n",
    "    for i, (metric, title) in enumerate(zip(['mnn', 'mnn_global', 'rho'], titles)):\n",
    "        for d, data in dataset_data.items():\n",
    "            axes[i].plot(x_axis_entries, data[metric], marker='o', linestyle='-', label=dataset_names.get(d, f\"{d}\"))\n",
    "        axes[i].set_title(title, fontsize=12)\n",
    "        axes[i].set_xlabel(x_axis_label, fontsize=12)\n",
    "\n",
    "    # Formatting\n",
    "    for ax in axes:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=10, width=0.5)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8, width=0.3)\n",
    "        ax.spines[\"bottom\"].set_linewidth(0.5)\n",
    "        ax.spines[\"left\"].set_linewidth(0.5)\n",
    "\n",
    "    axes[0].legend(fontsize=10)  # Only first plot has legend\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save & Show\n",
    "    save_path = os.path.join(figures_dir, f\"{plot_name}_3_quality_measures.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot KL Divergences 2x2 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kl_divergences_grid(embedding_dict, file_start):\n",
    "    \"\"\"\n",
    "    Plots averaged KL divergence curves (in this case, single-run curves) for each learning rate,\n",
    "    separately for each dataset in a 2x2 grid.\n",
    "\n",
    "    For each dataset:\n",
    "      - Extracts the KL divergence records (a dict mapping iteration -> error) for each learning rate.\n",
    "      - Sorts the learning rates (numeric ones first, then any non-numeric).\n",
    "      - Plots the KL divergence curve for each learning rate with the legend label using the Greek letter η.\n",
    "    \n",
    "    Parameters:\n",
    "      embedding_dict (dict): Dictionary with keys (η, dataset_index) mapping to \n",
    "                             (embedding, labels, kld_values) where kld_values is a dict mapping iteration -> error.\n",
    "      file_start (str): e.g eta, ee etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine unique dataset indices.\n",
    "    datasets = sorted({ds for (_, ds) in embedding_dict.keys()})\n",
    "    \n",
    "    # Optional: map dataset indices to names.\n",
    "    dataset_names = {0: \"Iris\", 1: \"Macosko\", 2: \"MNIST\", 3: \"Flow18\"}\n",
    "    \n",
    "    # Create a 2x2 grid of subplots.\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over each dataset to plot its KL divergence curves.\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Collect learning rates for this dataset.\n",
    "        ee_factors = [alpha for (alpha, ds) in embedding_dict.keys() if ds == dataset]\n",
    "        # Order learning rates: numeric first (sorted), then any non-numeric - this isn't strictly necessary\n",
    "        #numeric_lrs = sorted([eta for eta in lrs if isinstance(eta, (int, float))])\n",
    "        #non_numeric_lrs = [eta for eta in lrs if not isinstance(eta, (int, float))]\n",
    "        #ordered_lrs = numeric_lrs + non_numeric_lrs\n",
    "        \n",
    "        # Plot each learning rate's KL divergence curve.\n",
    "        for alpha in ee_factors:\n",
    "            _, _, kld_values = embedding_dict[(alpha, dataset)]\n",
    "            # Sort iterations and get corresponding error values.\n",
    "            iterations = sorted(kld_values.keys())\n",
    "            errors = [kld_values[it] for it in iterations]\n",
    "            ax.plot(iterations, errors, linestyle='-', label=f\"EE length = {alpha}\")\n",
    "        \n",
    "        # Format the subplot.\n",
    "        ax.set_xlabel(\"Iteration\", fontsize=12)\n",
    "        ax.set_ylabel(\"KL Divergence\", fontsize=12)\n",
    "        title = dataset_names.get(dataset, str(dataset))\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=10, width=0.5)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8, width=0.3)\n",
    "        \n",
    "        if idx==0: \n",
    "          ax.legend(fontsize=10)\n",
    "    \n",
    "    # Hide any unused subplots (if fewer than 4 datasets).\n",
    "    for j in range(len(datasets), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    # Adjust spacing between subplots.\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    \n",
    "    # Save and display the figure.\n",
    "    save_path = os.path.join(figures_dir, f\"{file_start}_kl_divergences_grid.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Plotting for Different Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_quality_measures(embedding_dict, X, ee_lengths):\n",
    "    \"\"\"\n",
    "    Computes the average quality measures across different seeds for each learning rate\n",
    "    and stores individual seed results.\n",
    "\n",
    "    Parameters:\n",
    "        embedding_dict (dict): Dictionary with (ee_iter, seed) as keys and\n",
    "                               (embedding, labels, kld_values) as values.\n",
    "        X (numpy.ndarray): Original high-dimensional data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with learning_rate as keys and values being another dictionary:\n",
    "              {\n",
    "                  \"average\": (mnn_avg, mnn_global_avg, rho_avg),\n",
    "                  \"seeds\": {seed: (mnn, mnn_global, rho), ...}\n",
    "              }\n",
    "    \"\"\"\n",
    "    quality_results = {}\n",
    "\n",
    "    for ee_len in ee_lengths:\n",
    "        mnn_list, mnn_global_list, rho_list = [], [], []\n",
    "        seed_results = {}\n",
    "\n",
    "        for seed in [key[1] for key in embedding_dict.keys() if key[0] == ee_len]:\n",
    "            # Unpack three elements; ignore kld_values\n",
    "            embedding, labels, _ = embedding_dict[(ee_len, seed)]\n",
    "            mnn, mnn_global, rho = quality_measures.embedding_quality(X, embedding, labels)\n",
    "\n",
    "            mnn_list.append(mnn)\n",
    "            mnn_global_list.append(mnn_global)\n",
    "            rho_list.append(rho)\n",
    "\n",
    "            # Store per-seed results\n",
    "            seed_results[seed] = (mnn, mnn_global, rho)\n",
    "\n",
    "        # Compute averages\n",
    "        quality_results[ee_len] = {\n",
    "            \"average\": (\n",
    "                np.mean(mnn_list),\n",
    "                np.mean(mnn_global_list),\n",
    "                np.mean(rho_list)\n",
    "            ),\n",
    "            \"seeds\": seed_results\n",
    "        }\n",
    "\n",
    "    return quality_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quality_measures_seeds(quality_results, x_axis_label=\"EE length\", plot_name=\"ee_length_seeds\"):\n",
    "    \"\"\"\n",
    "    Plots three side-by-side plots for the three embedding quality measures.\n",
    "    Includes individual seed results as connected grey lines and averaged results in black.\n",
    "\n",
    "    Parameters:\n",
    "        quality_results (dict): Dictionary with ee_length as keys and values being another dictionary:\n",
    "                                {\n",
    "                                    \"average\": (mnn_avg, mnn_global_avg, rho_avg),\n",
    "                                    \"seeds\": {seed: (mnn, mnn_global, rho), ...}\n",
    "                                }\n",
    "        x_axis_label (str): Label for the x-axis (default \"EE length\").\n",
    "        plot_name (str): Name for saving the file.\n",
    "        figures_dir (str): Directory where the plot will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get sorted EE lengths and create x-axis labels.\n",
    "    ee_lengths = sorted(quality_results.keys())\n",
    "    ee_labels = [str(x) for x in ee_lengths]\n",
    "\n",
    "    # Prepare averaged data for plotting.\n",
    "    mnn_values = [quality_results[x][\"average\"][0] for x in ee_lengths]\n",
    "    mnn_global_values = [quality_results[x][\"average\"][1] for x in ee_lengths]\n",
    "    rho_values = [quality_results[x][\"average\"][2] for x in ee_lengths]\n",
    "\n",
    "    # Prepare per-seed data: For each seed, collect quality measures across EE lengths.\n",
    "    seed_data = {}  # key: seed, value: dict with lists for each measure.\n",
    "    for x in ee_lengths:\n",
    "        seed_results = quality_results[x][\"seeds\"]\n",
    "        for seed, (mnn, mnn_global, rho) in seed_results.items():\n",
    "            if seed not in seed_data:\n",
    "                seed_data[seed] = {'mnn': [], 'mnn_global': [], 'rho': []}\n",
    "            seed_data[seed]['mnn'].append(mnn)\n",
    "            seed_data[seed]['mnn_global'].append(mnn_global)\n",
    "            seed_data[seed]['rho'].append(rho)\n",
    "\n",
    "    # Create the 1x3 subplot layout.\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    titles = [\"KNN\", \"KNC\", \"CPD\"]\n",
    "\n",
    "    # Plot per-seed results (grey lines) for each quality measure.\n",
    "    for seed, data in seed_data.items():\n",
    "        axes[0].plot(ee_labels, data['mnn'], linestyle='-', color='grey', alpha=0.5)\n",
    "        axes[1].plot(ee_labels, data['mnn_global'], linestyle='-', color='grey', alpha=0.5)\n",
    "        axes[2].plot(ee_labels, data['rho'], linestyle='-', color='grey', alpha=0.5)\n",
    "\n",
    "    # Plot averaged results (black with markers) for each quality measure.\n",
    "    axes[0].plot(ee_labels, mnn_values, marker='o', linestyle='-', color='black', label=\"Average\")\n",
    "    axes[1].plot(ee_labels, mnn_global_values, marker='o', linestyle='-', color='black', label=\"Average\")\n",
    "    axes[2].plot(ee_labels, rho_values, marker='o', linestyle='-', color='black', label=\"Average\")\n",
    "\n",
    "    # Set x-axis and y-axis labels.\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(x_axis_label, fontsize=12)\n",
    "    #axes[0].set_ylabel(\"KNN\", fontsize=12)\n",
    "    #axes[1].set_ylabel(\"KNC\", fontsize=12)\n",
    "    #axes[2].set_ylabel(\"CPD\", fontsize=12)\n",
    "\n",
    "    # Set subplot titles.\n",
    "    for i, title in enumerate(titles):\n",
    "        axes[i].set_title(title, fontsize=12)\n",
    "\n",
    "    # Formatting: remove top/right spines and adjust tick parameters.\n",
    "    for ax in axes:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=10, width=0.5)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8, width=0.3)\n",
    "        ax.spines[\"bottom\"].set_linewidth(0.5)\n",
    "        ax.spines[\"left\"].set_linewidth(0.5)\n",
    "\n",
    "    # Only include legend in the first plot.\n",
    "    axes[0].legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and display the figure.\n",
    "    save_path = os.path.join(figures_dir, f\"{plot_name}_3_quality_measures.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datasets.load_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing a smaller number of datapoints \n",
    "# n_points = 200\n",
    "# all_data = [datasets.load_n_samples(n_points, X, y) for (X, y) in all_data]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wissrech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
