\chapter{Theoretical Results}\label{chapter:theoretical_results}
\textcolor{red}{TODO: maybe say something about the importance of theoretical work - can lead to better understanding and new directions for improving the algorithm}

t-SNE has been used a lot in practice and there is comparatively less theoretical material available. 
In this chapter, we will first give an overview of the theoretical work on t-SNE that has been done in the last couple of years and then look at a couple of results of interest in more detail. 

\section{Literature Review}
This section is inspired by \cite{murray2024largedatalimitsscaling}. 

Early theoretical work has been focused on establishing clustering guarantees for t-SNE. 
\textcolor{red}{To the best of my knowledge}, Shaham and Steinerberger started this line of investigation with their 2017 paper \enquote{Stochastic Neighbor Embedding Separates Well-Separated Clusters} \cite{shaham17}. 
They proved a clustering result for the precursor algorithm SNE, but their result has been criticised since it is only nontrivial when the number of clusters is significantly larger than the number of points per clusters, which is an unrealistic assumption for most datasets t-SNE is commonly used on, see \cite{Arora18}.  
Linderman and Steinerberger \cite{LinStei22} use a dynamical systems approach for understanding t-SNE. 
They understand the embedding points as particles experiencing attractive and replusive forces and formulate a shrinkage result for the diameter of embedded clusters based on the EE exaggeration factor and gradient descent step size. \\
Building on the preprint of \cite{LinStei22}, \cite{Arora18} point out that Linderman's result does not rule out the possibility of multiple clusters merging into one. 
\cite{Arora18} gives a first formal framework for the problem of data visualization and formulates an improved clustering guarantee for some definition of spherical and well-separated clusterable data. 
However, both the clustering guarantees in \cite{Arora18} and \cite{LinStei22} have been criticised for making assumptions that do not hold in practice, see \cite{yang2021tsneoptimizedrevealclusters}. In our experiments section, we will therefore also investigate these assumptions ourselves. 


Studying t-SNE in terms of attraction-repulsion dynamics has proven to be one of the most fruitful directions for obtaining theoretical results on t-SNE. 
Steinerberger and Zhang explore coloring of t-SNE plots by the direction of forces acting on each point as a means of obtaining additional information on the embedding \cite{SteiZhang22}. \textcolor{red}{Does this belong here actually?}

In \cite{Cai22}, an asymptotic equivalence of the EE phase with power iterations in spectral embeddings is established. 
This means that for strongly clustered data, one can replace the EE phase with a spectral embedding, thereby speeding up the process. 
They also investigate the embedding phase and characterize it into an amplification and a stabilization phase. 
Practical implications of their work include: 
\begin{itemize}
    \item stopping EE early for noisy data to avoid overshooting (they suggest $K_0 = \lfloor (\log n)^2 \rfloor$ iterations)
    \item the observation that t-SNE is reliable in terms of cluster membership but not relative position of clusters 
    \item the observation that false clustering may occur - one should thus run t-SNE multiple times if possible
\end{itemize}
More recently, the focus of theoretical research on t-SNE has shifted to the question of equilibrium distributions and convergence in the large data limit ($n \to \infty$). 
\cite{murray2024largedatalimitsscaling} shows that standard t-SNE embeddings do not have a consistent limit as $n \to \infty$ and proposes a rescaled model with a consistent limit which mitigates the asymptotic decay of the attractive forces. 

\section{Clustering Guarantees}
In this section, we will present the result in \cite{LinStei22} and \textcolor{red}{TODO: maybe investigate whether the assumptions hold for real datasets}. 

The following assumptions are made: 
\begin{enumerate}
    \item $\mathcal{X}$ is clusted. We assume that there exists a number of clusters $k \in \mathbb{N}$ and a map $\pi: \{1,\dots,n\} \to \{1,\dots,k\}$ which maps each point to a cluster, such that the following holds: if $\pi(x_i) = \pi(x_j)$, then \begin{equation}
    \label{eq:4.1}
        p_{ij} \geq \frac{1}{10 n |\Omega(i)|}
    \end{equation}
    where $\Omega(i)$ is the size of the cluster in which $x_i$ and $x_j$ lie. 
    \item Parameter choice. Step-size $h$ and exaggeration parameter $\alpha$ are chosen such that for some $1\leq i \leq n$, 
    \begin{equation}
        \frac{1}{100} \leq \alpha h \sum_{j \neq i, \pi(i) = \pi(j)} p_{ij} \leq \frac{9}{10}
    \end{equation}
    \item Localized initialization. The initialization of the low-dimensional embedding satisfies $\mathcal{Y} \in [-0.01, 0.01]^2$. 
\end{enumerate}
The main result is applicable to a single cluster. 

\begin{thm}[\cite{LinStei22}]
    Under assumptions (i)-(iii), the diameter of the embedded cluster $\{y_j: 1 \leq j \leq n \land \pi(i) = \pi(j)\}$ decays exponentially (at universal rate) until its diameter satisfies, for some constant $c > 0$,
    \begin{equation}
        \text{diam} \{y_j: 1 \leq j \leq n \land \pi(i) = \pi(j)\} \leq c \cdot h \left(\alpha \sum_{j \neq i \pi(i) = \pi(j)} p_{ij} + \frac{1}{n} \right). 
    \end{equation}
\end{thm}
\textcolor{red}{Should I include a proof? Or at least its idea?}

There are several problems with this result however. 
\begin{itemize}
    \item the assumptions on $h$ and $\alpha$ do not really make sense with what has been established empirically. In this paper, they suggest $h=1$, whereas later research has suggested values like $h=200$ or higher for large datasets. 
    \item There is this paper which says that condition (i) does not hold for datasets in practice even if they are clusterable. Steinerman says it's only a loose condition, but this doesn't seem to actually be the case. 
\end{itemize}

\textcolor{red}{TODO: quickly mention \cite{Arora18} which has improved on this result a bit}

\section{Large Data Limits}
Theoretical Literature on t-SNE has naturally not focused on what happens when we let the number of data points go to infinity. After all, real-world datasets have a finite number of points. It is however an interesting questions, that allows us to get deeper insight into the inner workings of t-SNE and is explored in \cite{murray2024largedatalimitsscaling}. 

\subsection*{Setup}
For this section, instead of assuming a given dataset, we have a probability distribution $\mu \in \mathcal{P}(\Omega)$ on $\mathbb{R}^d$ with support on $\Omega$, a bounded and $C^2$ domain. 
We further assume that $\mu$ has a bounded density function $\rho(x)$ with respect to the Lebesgue measure. 
Our points $X_1, \dots, X_n$ are then drawn independently from $\mu$ and the $p_{ij}$ and $q_{ij}$ values are calculated as always. 
If $n$ datapoints are drawn, we define the matrices $P_n = (p_{ij})_{i,j=1}^n$ and $Q_n = (q_{ij})_{i,j=1}^n$. 

Since we want to study large data limits, we do not consider the KL-divergence between $P_n$ and $Q_n$ directly, but instead define a closely related functional 
\begin{equation}
    \text{KL}_n (T) = \sum_{i,j} p_{ij} \log \frac{q_{ij}}{q_{ij}(T)}
\end{equation}
where 
\begin{equation}
    q_{ij}(T) \coloneq \frac{(1+ |T(X_i) - T(X_j)|^2)^{-1}}{\sum_{k \neq l} (1+ |T(X_k) - T(X_l)|^2)^{-1}}
\end{equation}
and $T: \mathbb{R}^d \to \mathbb{R}^m$. This is now technically not a KL-divergence anymore. 

Building on the view of t-SNE in terms of attraction-repulsion dynamics, \cite{murray2024largedatalimitsscaling} splits up the t-SNE objective function into an attractive term $A_n[T]$, a repulsive term $R_n[T]$ and a purely data-dependent term  $D_n$ (which plays no role during gradient descent). Defining 
\begin{equation}
    A_n[T] \coloneq \frac{1}{n} \sum_{i=1}^n \frac{\sum_{j=1}^n \exp(-|X_i - X_j|^2/(2\sigma_i^2)) \log(1+ |T(X_i) - T(X_j)|^2)}{\sum_{j=1}^n \exp(-|X_i - X_j)^2 / (2\sigma_i^2)}
\end{equation}
and 
\begin{equation}
    R_n[T] \coloneq \log \left( \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \frac{1}{1+ |T(X_i) - T(X_j)|^2} \right), 
\end{equation}
we can write $\text{KL}_n (T) = A_n[T] + R_n[T] + D_n$. 

This means, we have the following optimization problem
\begin{equation}
    {\arg \min}_{T: \mathbb{R}^d \to \mathbb{R}^m} \text{KL}_n (T) = {\arg \min}_{T: \mathbb{R}^d \to \mathbb{R}^m} A_n[T] + R_n[T].
\end{equation}

Again, we observe that there is a competition between minimizing the attractive term and minimizing the repulsive term. 

\subsection*{Motivation from Adaptive Kernel Density Estimation}

We can think of perplexity as ensuring an adaptive bandwidth. 
The autors of \cite{murray2024largedatalimitsscaling} point out that this is similar to a standard approach used in adaptive kernel density estimation. 
For a kernel $K: \mathbb{R}^d \to \mathbb{R}$, the quantity $\frac{1}{n h_n^d}\sum_{i=1}^n K(\frac{x-X_i}{h_n})$ is a pointwise consistent estimator of the density $\rho(x)$ as long as $\frac{h h_n^d}{\log(n)} \to \infty$. 

There is one problem however: they point out that in order to maintain a constant perplexity, we have to let $\sigma_{i,n} = \mathcal{O}(n^{-1/d})$ with a high probability. 
But, as is known in the world of kernel density estimation, this scaling does not lead to consistency of the estimator defined above. 
So instead, they allow perplexity to grow slowly with the number of samples $n$, at any rate $n^\alpha, \alpha > 0$. 

\subsection*{Results}

We now define an averaged version of the energies, where we replace the stochastic $\sigma_i$ with the deterministic $h_n \sigma_\kappa(x)$: 
\begin{equation}
    \tilde{A}_n[T] \coloneq  \int_{\Omega} \frac{\int_{\Omega} \exp(-|x - x'|^2/(2 h^2 \sigma_\kappa^2(x))) \log(1+ |T(x) - T(x')|^2) \rho(x')dx'}{\int_{\Omega} \exp(-|x - x'|^2/(2 h^2 \sigma_\kappa^2(x))) \rho(x')dx'} \rho(x)dx  
\end{equation}
and 
\begin{equation}
    \tilde{R}_n[T] \coloneq \log \left( \int \int_{\Omega \times \Omega} \frac{1}{1+ |T(x) - T(x')|^2} \rho(x) \rho(x')dx dx' \right), 
\end{equation}

\textcolor{red}{TODO: make the equations look nicer}

This first result states that the averaged t-SNE energy does not have a limiting solution. 
\begin{thm}[\cite{murray2024largedatalimitsscaling}]
    Let $T_n$ be a sequence of minimizers of the energies $\tilde{A}_{h_n}[T] + \tilde{R}[T]$. Then $T_n$ does not converge pointwise to any $T^* \in L^\infty(\Omega, \mathbb{R}^m)$. 
\end{thm}

For details of the proof, we refer to \cite{murray2024largedatalimitsscaling}. However, the main idea again is to consider how attraction repulsion dynamics change as the number of samples grow. 
They prove that the attractive term will shrink to zero as $n \to \infty$, which means that for large sample sizes, the repulsive force dominates. This will then result in unbounded embeddings. 

Note that this also shows that early exaggeration \enquote{is not merely heuristic but is, in fact, asymptotically consistent with the underlying behavior of the algorithm}.

Taking the above theorem as motivation, \cite{murray2024largedatalimitsscaling} propose a new, rescaled version of t-SNE, where we do not only apply exaggeration in the early iterations of the algorithm, but instead multiply the entire attractive force by a sequence $1/h_n^2$, where we  require that $h_n \to 0$ and $\frac{n /h_n^d}{\log(n)} \to \infty$ as $n \to \infty$. 
The rescaled model for $n$ samples thus minimizes the loss function 
\begin{equation}
    \hat{\text{KL}}_n(T) = \frac{A_n [T]}{h_n^2} + R_n[T].
\end{equation}

This energy will the converge (for a fixed $T$) towards the limiting energy 
\begin{equation}
    \mathcal{K}\mathcal{L} (T) \coloneq \frac{\kappa^{2/d}}{2 \pi e} \int_\Omega \sum_{i=1}^m |\nabla T_i(x)|^2 \rho^{1-2/d} (x) d x + \log\left( \iint_{\Omega \times \Omega} \frac{\rho(x)\rho(x')}{1+ |T(x)-T(x')|^2}  d x d x' \right), 
\end{equation}

which behaves consistently as shown by the following theorem. 

\begin{thm}[\cite{murray2024largedatalimitsscaling}]
    Let $\mu$ be a distribution supported on a compact, $C^1$ domain $\Omega \subset \mathbb{R}^d$ with Lebesgue density bounded above and below on $\Omega$. 
    Then, for every $T \in C^2(\Omega; \mathbb{R}^m)$ we have $\lim_{n \to \infty} \hat{\text{KL}}_n(T) \to \mathcal{K}\mathcal{L} (T)$
    and there exists $T^* \in H^1(\Omega, \rho)$ for which $\mathcal{K}\mathcal{L} (T^*) = \inf_{T: \Omega \to \mathbb{R}^m} \mathcal{K}\mathcal{L} (T)$.  
\end{thm}

\textcolor{red}{TODO: check whether assumptions really hold that way and also what is it with the limit should it be equal to or tend to the limit?, also mention that the results are for the deterministic, not stochastic form}

They say in this paper that this new approach provides a more stable and consistent framework. 