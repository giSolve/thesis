\section{Single-level Monte Carlo quadrature}
We introduce our method first for the single-level case.
Consider nonlinear eigenvalue problems as in \Cref{chapter:cim} now for a parametrized family of functions $T_{\bs{y}} \in H(\Omega, \bC^{m \times m})$ for $\bs{y}\in [-1,1]^{\bN}$, where we take $T_\mathbf{0}$ as the reference element.
Let $k$ be the number of eigenvalues, including multiplicities, inside of the contour $\Gamma$.
We write $A_{\bs{y}, 0}$ and $A_{\bs{y}, 1}$ if \cref{eq:A0,eq:A1} are computed for $T_{\bs{y}}$, respectively.

%For each sample $\bfy \in [-1,1]$, we want to find a suitable quantity to compute for which a meaningful expectation can be found.
We recall \Cref{section:reduction-cim}, where the method to solve nonlinear eigenvalue problems was presented.
After calculating the matrices $A_{\bs{y}, 0}$ and $A_{\bs{y}, 1}$, we computed the eigenvalue decomposition 
\begin{equation}
    \label{eq:svd-A0-param}
    A_{\bs{y}, 0} = V_{\bs{y},0} \Sigma_{\bs{y},0} W_{\bs{y}, 0}\hm.
\end{equation}
%\comment{pensar si quiero resumir aquí brevemente la herleitung, bzw. poner tipo la fórmula de B y de S}.
Now, the central idea of the method is to fix two orthogonal matrices $V_0 \in \bC^{m\times k}$ and $W_0 \in \bC^{l\times k}$, which will replace $V_{\bs{y},0}$ and $W_{\bs{y}, 0}$.
Instead of the singular value decomposition \eqref{eq:svd-A0-param}, we then take
\begin{equation}
    A_{\bs{y}, 0} = V_0 \Sigma_{\bs{y},0} W_0\hm,
\end{equation}
where $\Sigma_{\bs{y},0} = V_0\hm A_{\bs{y}, 0} W_0$ is not necessarily a diagonal matrix anymore.
We see that we can still perform each step of the reduction with this modified decomposition, as the only property needed of $V_{\bs{y},0}$ was that it be orthogonal, so we get
\[ 
    B_{\bs{y}} = V_0\hm A_{\bs{y},1} W_0 \Sigma_{\bs{y},0}\inv = S_{\bs{y}}\Lambda_{\bs{y}} S_{\bs{y}}\inv = V_0\hm V_{\bs{y}} \Lambda_{\bs{y}} (V_0\hm V_{\bs{y}})\inv.
\]
%\comment{y poner aquí las fórmulas de B y S otra vez pero con el $V_0$ fijado}
So, essentially, the major modification is that the matrix of eigenvectors of $B$ is given by $S_{\bs{y}} = V_0\hm V_{\bs{y}}$ instead of $S_{\bs{y}} = V_{\bs{y},0}\hm V_{\bs{y}}$, which means that we are always relating the matrix of eigenvectors of $T_{\bs{y}}$ that is given by $V_{\bs{y}}$ to the same orthogonal matrix $V_0$ instead of taking a different one for each sample.
%\comment{revisar/mejorar esto de qué es la idea}
Then, we compute the expectation of the matrix $B$, which has the same eigenvalues including multiplicities as our original problem, and from which we can directly derive eigenvectors for our original problem, see \Cref{thm:equiv-B-T(z)}.
We summarize the method in \Cref{alg:single-level}.
%\comment{add better explanation of why we do this, why this is a suitable größe}

Let $l \geq k$.
\begin{algorithm}
    \DontPrintSemicolon

    \SetKwInput{Input}{Input}
    \SetKwInput{Output}{Output}
    \SetKw{KwGoTo}{go to}

    \Input{A family $T_{\bs{y}} \in H(\Omega, \bC^{m \times m})$ of parametrized matrices and a contour $\Gamma \subset \Omega$}
    %\Output{An approximation of $\bE[B]$, where $B$... (retains eigenvalues and mulitplicity o así)}
    \Output{An approximation of $\bE[B]$}
    \BlankLine
    Draw a random matrix $\widehat{V} \in \bC^{m\times l}$\;
    Choose two orthogonal matrices $V_0$ and $W_0$ (e.g. using the SVD of $A_{\mathbf{0},0})$\;
    \For{$i = 1,\ldots,M$}{
        Draw a random sample $\bs{y}_i \sim \text{Unif}([-1,1]^N)$\;\label{line:monte-carlo-y}
        Compute $A_{\bs{y}_i, 0}$ and $A_{\bs{y}_i, 1}$\;
        Set $\Sigma_{\bs{y}_i,0} = V_0 A_{\bs{y}_i,0} W_0\hm$\;
        Set $B_{\bs{y}_i} = V_0\hm A_{\bs{y}_i,1} W_0 \Sigma_{\bs{y}_i,0}\inv$\;
    }
    \Return{$B = \frac{1}{M} \sum_{i=1}^M B_{\bs{y}_i}$}
    \caption{Single-level Monte Carlo quadrature for nonlinear eigenvalue problems}\label{alg:single-level}
\end{algorithm}
\begin{rem}
    (a) As $V_0$ and $W_0$ are not the matrices associated to the eigenvalue decomposition of $A_{\bfy_i, 0}$, the matrix $\Sigma_{\bfy_i, 0}$ is not necessarily diagonal anymore.
    Therefore, one should not invert the matrix directly.
    Instead, it is possible to compute $(W_0 \Sigma_{\bfy_i,0}\inv)\tp$ as the solution $X\tp$ of the system of linear equations $\Sigma_{\bfy_i,0}\tp X\tp = W_0\tp$.

    (b) In the current form, the algorithm performs a Monte Carlo quadrature (see \Cref{line:monte-carlo-y}).
    It is also possible to replace it by a quasi-Monte Carlo method. %\comment{igual meto directamente la quasi Monte Carlo methode en el algoritmo, pero entonces sería un poco más unübersichtlich}
\end{rem}