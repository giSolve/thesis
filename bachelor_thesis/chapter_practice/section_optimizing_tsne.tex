\section{Optimizing t-SNE Hyperparameters}

\subsection{Automated Stopping of EE and t-SNE Run}
The original t-SNE algorithm was widely viewed as having a scaling problem. Not only due to its $\mathcal{O}(N^2)$ runtime, but also because the embeddings became meaningless for large datasets. 
One simply would not be able to see anything other than lots of points on top of each other. 
While the former problem was addressed with the development of Barnes-Hut t-SNE \cite{vdMaa14} and FIt-SNE \cite{Lin19}, the latter is addressed in \cite{belkina19}. 

One problem of standard software packages for t-SNE is that there is a default number of iterations (both for the early exaggeration phase as well as for the overall embedding). 
Of course this is beneficial for practitioners who do not have a lot of experience with the algorithm, but it does suggest that there is a \enquote{one size fits all} solution. 
In reality though \textcolor{red}{large data sets may need more iterations --- is this really true? I should test this!}. 

In order to solve this problem, \cite{belkina19} propose an automated approach to stopping the EE phase and the embedding. 
They observed the change in KL divergence to plateau after a certain about of iterations. 
So they track the relative rate of KL divergence change \begin{equation}\text{KLDRC}_N = \frac{\text{KLD}_{N-1} - \text{KLD}_N}{\text{KLD}_{N-1}}
\end{equation} where $N$ is the current iteration number. 
The idea then is to identify the local maximum of this KL divergence relative change and stop the RR phase at the next iteration. 
This can be done since we can calculate the KL divergence and its change at every iteration. 

They suggest to stop the algorithm entirely once 
\begin{equation}
  \text{KLD}_{N-1} - \text{KLD}_{N} < \text{KLD}_N / X 
\end{equation}
where $X=5000$ is suggested for cytometry data. 

\subsection{Informative Initialization}
The standard t-SNE algorithm starts with an initization $y_i^{(0)}$ for $i=\{1, 2, \dots, n\}$ which are drawn independently from $\mathcal{N}(0, \delta^2 I)$ for some small $\delta > 0$, see \cite{vdMaa08} and \cite{vdMaa14}. 

However, recent work \cite{kobak21} has shown that informative initialization leads to embeddings that better preserve large-scale structures within the data. 
They argue that using informative intialization should be the default option for t-SNE.
Indeed, modern implementations of t-SNE in libraries like openTSNE \cite{openTSNE} or Scikit-learn now all use PCA initialization by default. 
This means that we perform a principal component analysis on the input data $x_1, \dots, x_n$ and use the output $y_1, \dots, y_n$ as the initial points for the low-dimensional embedding. 

In our experiments, we do not necessarily see a visual improvement of the PCA initialized embedding over the random initialization one, but \textcolor{red}{one should look at other metrics to see how well the global structure is preserved e.g. Pearson correlation or Spearman correlation}. 

\subsection{Current State of Literature of Parameter Choices}
Here, I want to collect guidelines for choosing t-SNE parameters. It could be interesting to look at this from the perspective of a practitioner, using t-SNE libraries like openTSNE or the scikit-learn implementation and investigate if the default choices lead to good results. 

\begin{itemize}
  \item from \cite{openTSNE}: one easy way to improve the global consistency of t-SNE embeddings is to increase the sizes of the neighborhoods considered for constructing the embedding, i.e. to \textbf{increase perplexity}. But unfortunately, this often leads to a loss in local structure and tends to obsure small clusters. \cite{KoBe19SingleCell} instead propose to use a \textbf{mixture of Gaussians} to better preserve short-range as well as long-range distances and get a better balance 
  \item on \textbf{perplexity}: \cite{belkina19} suggest used 30 to 50 and find that the exact value does not impact the embedding too much
  \item on \textbf{early exaggeration}: \cite{belkina19} suggest using 4 to 12 and find that the exact value does not impact the embedding too much 
  \item a feature of t-SNE is that is tries to use all available space. this means that clusters are often only separated by thin boundaries, which can become a problem for large datasets. \cite{BoehmBerens22} showed that exaggeration even after EE (\textbf{late exaggeration}) can lead to embeddings that are similar to UMAP and create more white space between clusters 
  \item if we want to look at different levels of resolution, \cite{Ko20HeavyTails} suggests that we can vary the \textbf{degrees of freedom} in the $t$-distribution used. Even heavier-tailed distributions than the Cauchy distribution can bring out smaller clusters. 
  \item \textbf{learning rate}: \cite{belkina19} suggest $\eta = N/\rho$ where $\rho$ is the EE factor, this is used by default in openTSNE. 
\end{itemize}

