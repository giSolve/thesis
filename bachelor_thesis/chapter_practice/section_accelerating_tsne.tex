\section{Accelerating t-SNE}

The original t-SNE algorithm is not very fast. Its runtime is $\mathcal{O}(N^2)$, which quickly becomes infeasible for datasets with $100,000$ or more points \textcolor{red}{check the exact number} \comment{explain more why this runtime}. In this section, we will give an overview of the two most popular methods proposed to speed up the algorithm. 

\subsection*{Barnes-Hut t-SNE}
This method uses tree-bases algorithms to speed up t-SNE and was proposed by van der Maaten in 2014 \cite{vdMaa14}. 

\begin{itemize}
    \item the gradient of the t-SNE loss function (KLD) has a natural interpretation as an $N$-body system in which all of the $N$ points in the low-dimensional embedding exert forces on each other. We then need to compute the resultant force on each of the points
\end{itemize}

%\title{Approximating Attractive Forces}
Computing the attractive force is not too expensive computationally, if we approximate input similarities and use vantage-point trees. Recall that input similarities $p_{ij}$ are computed based on a Gaussian kernel. 
Thus, $p_{ij}$ values corresponding to dissimilar input objects $x_i$ and $x_j$ are very small. 
So, it makes sense to develop a sparse approximation for the $p_{ij}$. Instead of computing $N^2$ pairwise similarities, we focus on the $\lfloor 3 \kappa \rfloor$ nearest neighbors of each of the $n$ input objects only, where $\kappa$ denotes the perplexity. We denote the nearest neighbor set of $x_i$ by $\mathcal{N}_i$. 

The similarities are thus given by 
\begin{equation}
    p_{j|i} = \begin{cases}
    \frac{\exp(-\norm{x_i - x_j}^2) / 2 \sigma_i^2}{\sum_{k \in \mathcal{N}_i} \exp(-\norm{x_i - x_k}^2) / 2 \sigma_i^2} & \text{ if } j \in \mathcal{N}_i \\
    0  & \text{ otherwise}
    \end{cases}
\end{equation}
which are again symmetrized then. 

We can find the nearest neighbor sets $\mathcal{N}_i$ in $\mathcal{O}(u N \log N)$ time by building a data structure called a vantage-point tree and performing nearest neighbor search with its help. For details, see \cite{vdMaa14}. 

\textcolor{red}{TODO: maybe say more about vantage-point trees here?}

%\title{Approximating Repulsive Forces}
Naively computing the repulsive forces is not a good idea. It would be in $\mathcal{O}(N^2)$. 
Using the Barnes-Hut algorithm however, this can be sped up to $\mathcal{O}(N \log N)$. 
The algorithm relies on the observation that the repulsive forces exerted between two small groups of points are very similar whenever these two groups are relatively far away from each other. 
More precisely, if we consider points $y_i, y_j$ and $y_k$ with $\norm{y_i - y_j} \approx \norm{y_i - y_k} \gg \norm{y_j - y_k}$, then the contributions of $y_j$ and $y_k$ to $F_{\text{rep}, i}$ will be roughly equal. 

To take advantage of this fact, the Barnes-Hut algorithm constructs a data structure known as a quadtree (or for three-dimensional embeddings an octtree). 
Once we have built a quadtree on the current embedding, we traverse it via depth-first search and decide at every node of the quadtree, whether the corresponding cell can be used as a summary for the contributions to $F_{\text{rep}}$ of all points in the cell or if we need to go deeper. 

\textcolor{red}{Maybe insert a picture of a Quadtree here? But I am not sure how much to write about Barnes-Hut really...}

\subsection*{Fast Interpolation-Based t-SNE}
As observed in \cite{KoBe19SingleCell}, the Barnes-Hut implementation of t-SNE, while faster than the originally proposed algorithm, still becomes provibitively slow for datasets with $n \gg 100,000$. 
The FFT accelerated, fast interpolation-based version of t-SNE proposed by Linderman et. al. in 2019 \cite{Lin19} attempts to further speed up the computation of t-SNE embeddings with a special view towards applications in the analysis of single-cell RNA-seq data, which keeps growing and datasets often have north of 1 million datapoints. 

Rough notes from \cite{Lin19}: 
\begin{itemize}
    \item the repulsive force acting on each point can be expressed via sums of the form \begin{equation}
        \phi(y_i) = \sum_{j=1}^N K(y_i, y_j) q_j 
    \end{equation}
    with either $K_1(y,z) = \frac{1}{1+\norm{y-z}^2}$ or $K_2(y,z) = \frac{1}{(1+\norm{y-z}^2)^2}$ (both of these are smooth and translation-invariant kernels). 
    \item key idea: instead of computing all these sums (this would be in $\mathcal{O}(N^2)$), we instead use polynomial interpolation to approximate the kernels $K$ 
    \item interpolation strategy: subdivide the embedding space into a number of intervals (in the 1D case) or squares (in the 2D case)
    \item in each interval, choose a small, fixed number $p$ of equispaced interpolation nodes 
    \item approximate the kernel $K$ by its low-order polynomial interpolant $K_p$ over these nodes 
    \item This replaces the direct kernel evaluation between every pair with an evaluation on a much coarser grid.
\end{itemize}
TLDR: Low-order polynomial interpolation mediates interactions through a fixed number of nodes, FFT exploits Toeplitz structure to accelerate convolution sums, so we get a near linear runtime. 


% algorithm pseudocode here
\begin{algorithm}[H]
    \caption{FFT-accelerated Interpolation-based t-SNE (FIt-SNE)}
    \label{alg:fit-sne}
    \KwIn{embedding points $\{y_i\}_{i=1}^N$, interpolation coefficients $\{q_i\}_{i=1}^N$, number of intervals $N_{\text{int}}$, interpolation points per interval $p$}
    \KwOut{$\varphi(y_i) = \sum_{i=1}^N K(y_i, y_j) q_j$ for $i=1, \dots, N$}


    For every interval $I_l$, form $p$ equispaced nodes $\tilde{y}_{j, l} = 1/(2N_{\text{int}}p) + \frac{j-1 + (l-1)p}{N_{\text{int}}p}$ for $j=1,\dots,p$. 

    \For{$I = 1$ \KwTo $N_{\text{int}}$}{
        Compute the coefficients $w_{m,l}$ given by 

        \[
            w_{m,l} = \sum_{y_i \in I_l} L_{m, \tilde{y}^l}(y_i) q_i, \text{      } m=1,\dots,p
        \]
    }

    Use FFT to compute values of $v_{m,n}$ given by 

        \[
            \begin{pmatrix}
                v_{1,1} & v_{2,1} & \dots & v_{p, N_{\text{int}}}
            \end{pmatrix}^T = \tilde{K} \begin{pmatrix}
                w_{1,1} & w_{2,1} & \dots & w_{p, N_{\text{int}}}
            \end{pmatrix}^T
        \]
    where $\tilde{K}$ is the Toeplitz matrix given by $\tilde{K}_{ij} = K(\tilde{y_i}, \tilde{y_j})$, $i, j = 1, \dots, N_{\text{int}}p$.
    
    \For{$I = 1$ \KwTo $N_{\text{int}}$}{
        Compute $\varphi(y_i)$ at all points $y_i \in I_l$ via 

        \[
            \varphi(y_i) = \sum_{j=1}^p L_{j, \tilde{y}^l}(y_i) v_{j,l}
        \]
    }

\end{algorithm}

\subsection*{Comparing Barnes-Hut and FIt-SNE}
When comparing the two methods, we only see minor differences in the embedding. 
However, the FIt-SNE version is many times faster than Barnes-Hut t-SNE, which is why we will be using FIt-SNE in all of our experiments going forward. 

As pointed out in \cite{openTSNE}, FIt-SNE scales linearly with the number of smaples, but it introduces additional computational overhead for each embedding.
This is often unneccessary for smaller datasets, which is why openTSNE uses Barnes-Hut t-SNE for data sets with fewer than $10,000$ samples and FIt-SNE for data sets with at least $10,000$ datapoints. 
%\begin{figure}[h]
  %  \centering 
  %  \includegraphics[width=\linewidth]{figures/BH_vs_FFT.png}
 %   \caption{Barnes-Hut versus FIt-SNE on the flow18 dataset}
 %   \label{fig:BH_vs_FIt-SNE}
%\end{figure}

